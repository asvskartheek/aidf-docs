{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AIDF Platform Documentation","text":"<p>Comprehensive reference for the Centific AI Data Foundry (AIDF) \u2014 an end-to-end platform for developing, deploying, and managing enterprise AI solutions. Covers all six sessions of the February 2026 training.</p> <ul> <li> <p> New here?</p> <p>Start with the hands-on walkthrough that takes you from zero to a running pipeline.</p> <p>First Steps \u2192</p> </li> <li> <p> Building an AI Service?</p> <p>Full code tutorial: from the abstract contract to a deployed classifier.</p> <p>Tutorial \u2192</p> </li> <li> <p> Managing infrastructure?</p> <p>Workspaces, projects, service deployment, and pipeline templates.</p> <p>Infra Hub \u2192</p> </li> <li> <p> Developer reference?</p> <p>Folder structure, strategy pattern, manifest, Docker, and onboarding.</p> <p>Developer Guide \u2192</p> </li> </ul>"},{"location":"#platform-overview","title":"Platform Overview","text":"<p>The AI Data Foundry (AIDF) is Centific's end-to-end platform for accelerating the development, deployment, and management of AI solutions. It serves as a comprehensive AI workflow orchestration environment that integrates data, models, and applications into one unified framework.</p> <p>AIDF's core purpose is to streamline every step of AI development \u2014 from high-quality data curation and preparation to model training, fine-tuning, and deployment \u2014 all under a single platform.</p>"},{"location":"#modular-architecture","title":"Modular Architecture","text":"<p>AIDF is designed with a modular, plugin-based architecture. Each component (called a \"studio\") is responsible for a different aspect of the AI lifecycle. The design allows teams to mix and match tools for specific use cases and scale independently.</p> <p>The platform leverages Kubernetes-based orchestration and containerization to deploy AI services at scale. Each AI function is packaged as an \"AI Service\" \u2014 a self-contained, deployable microservice exposing a standard HTTP endpoint.</p>"},{"location":"#six-core-modules","title":"Six Core Modules","text":"Module Role Data Hub Enterprise data catalog and metadata store. Manages data lineage, annotations, and human-in-the-loop tasks. AI Workbench Model catalog and development environment for benchmarking, fine-tuning, training, and governance. GenAI Studio Pipeline builder and AI services management hub. Orchestrates multi-step workflows with drag-and-drop. Agent Workbench Build, import, validate, and monitor agentic LLM workflows for business process automation. Infra Hub Infrastructure provisioning engine \u2014 manages workspaces, projects, compute, and AI service deployment. GenBI Natural-language cognitive analytics layer. Ask plain-English questions against structured data."},{"location":"#business-value","title":"Business Value","text":"<ul> <li>End-to-end efficiency: Covers every stage (ingestion, prep, training, evaluation, deployment, monitoring) under one platform. Includes 100+ pre-configured AI workflow templates.</li> <li>Quality of outcomes: Emphasizes high-quality data, model validation, RLHF feedback loops, integrated benchmarking, and drift monitoring.</li> <li>Scalability &amp; flexibility: Cloud-native microservice architecture handles multimodal AI workloads (vision, language, speech) in real-time. Infrastructure-agnostic across public cloud, private data centers, and edge devices.</li> <li>Reuse &amp; collaboration: Pipeline templates created by one team can be reused and scheduled for automated execution by others.</li> </ul>"},{"location":"#training-sessions","title":"Training Sessions","text":"<p>The AIDF platform training consisted of 6 sessions held February 3\u201311, 2026. Sessions 1\u20133 covered functional aspects; Sessions 4\u20136 covered technical deep-dives.</p> Session Date Theme Summary 1 Feb 3 Functional Platform Introduction &amp; Data Hub. Presenter Avinash Ganesh introduced all AIDF modules. Deep-dive into Data Hub, GenAI Studio pipeline builder with the Isaac Snapdragon pre-annotation pipeline example. 2 Feb 4 Functional AI Workbench in depth: model benchmarking, fine-tuning jobs, evaluation reports, AI Governance (Atlas/NIST AI RMF frameworks). 3 Feb 5 Functional Agent Workbench: live demo of MUFG corporate customer onboarding LangGraph workflow. Infra Hub: workspace/project management, compute sizing. 4 Feb 9 Technical AI Service creation: folder structure, Strategy Design Pattern, config management, manifest.xml, Docker basics. 5 Feb 10 Technical Model onboarding &amp; fine-tuning: training vs. inference services, compute offloading (RunPod/Denvr/Azure), MLFlow. Microsoft Phi-4 reference walkthrough. 6 Feb 11 Technical Pipeline template creation in GenAI Studio, conditional branching, environment promotion (Dev \u2192 QA \u2192 Staging \u2192 Prod)."},{"location":"first-steps/","title":"First Steps on AIDF","text":"<p>A hands-on walkthrough that takes you from zero to running your first AI pipeline on the Centific AI Data Foundry platform.</p> <p>Before you begin</p> <p>You need a Centific employee account. Ask your workspace admin to add you to the relevant project. For workspace creation you need the Platform Admin role.</p> <p>Choose your path:</p> Business User / AnalystAI Engineer / Developer <p>You want to understand what AIDF does, explore existing pipelines, and trigger annotation tasks \u2014 without writing code.</p> <p>Follow Steps 1 \u2192 2 \u2192 4 \u2192 7</p> <p>You'll build AI services, deploy them to the platform, wire them into pipelines, and run experiments.</p> <p>Follow all 7 steps</p>"},{"location":"first-steps/#step-1-understand-the-platform","title":"Step 1 \u2014 Understand the Platform","text":"<p>AIDF is made up of six modules, each handling a different stage of the AI lifecycle. Think of it as an assembly line: data flows in, gets processed, models get trained, then AI workflows get deployed to automate business processes.</p> <pre><code>Data Hub \u2192 AI Workbench \u2192 GenAI Studio \u2192 Agent Workbench\n        (supported by: Infra Hub + GenBI)\n</code></pre> <p>Three things you need to know:</p> <p>Everything is an \"AI Service\" Every capability on the platform \u2014 an OCR engine, a speech transcriber, a YOLO object detector \u2014 is packaged as a self-contained microservice with a standard HTTP endpoint. You build services, deploy them, and chain them together into pipelines.</p> <p>Workspaces \u2192 Projects \u2192 Services The hierarchy is: a Workspace maps to a cloud account/region (one per business unit or department), a Project is a specific initiative inside that workspace, and AI Services &amp; Pipelines live inside a project. You always work within a project context.</p> <p>Data flows from Hub \u2192 Canvas \u2192 Hub Raw data enters Data Hub. If it needs human labeling, it goes to Data Canvas (annotation platform). Labeled data returns to Data Hub. That labeled data then trains/fine-tunes models in AI Workbench, and the trained model powers an AI Service used in a GenAI Studio pipeline.</p> <p>Tip</p> <p>AIDF ships with 100+ pre-built pipeline templates covering common AI tasks. You don't need to build everything from scratch \u2014 explore the template library in GenAI Studio first.</p>"},{"location":"first-steps/#step-2-log-in-orient-yourself","title":"Step 2 \u2014 Log In &amp; Orient Yourself","text":"<p>AIDF uses your Centific employee credentials (SSO). No separate account needed.</p> <ul> <li> Navigate to the AIDF platform URL provided by your admin \u2014 you'll be redirected to Microsoft SSO login</li> <li> Sign in with your Centific email and password</li> <li> Check that your name and role appear in the top-left dropdown (Platform Admin or your assigned role)</li> </ul> <p>After login you'll see tiles for all six modules. Click any tile to enter that module. The most important one to start with is Infra Hub \u2014 it's where all workspace and project setup happens.</p> <p>Two views in Infra Hub: Use the top-left dropdown to switch:</p> <ul> <li>Admin View \u2014 manage workspaces &amp; projects</li> <li>Config View \u2014 deploy AI services &amp; pipelines</li> </ul> <p>Info</p> <p>If you can't see a module or get a permission error, you haven't been added to a project yet. Contact your workspace admin \u2014 they need to add your email with an appropriate role.</p>"},{"location":"first-steps/#step-3-set-up-your-workspace","title":"Step 3 \u2014 Set Up Your Workspace","text":"<p>Platform Admin role required</p> <p>Only admins can create workspaces. If you're an AI engineer or business user, skip to Step 4 \u2014 your admin will have already set this up.</p> <p>A workspace ties together a cloud account (e.g., your team's Azure subscription), a region (e.g., East US), and one or more environments (Dev, QA, Staging, Prod).</p> <pre><code>Workspace: MUFG-GCMD-Singapore (Azure East Asia)\n  \u2514\u2500 Environment: Dev (Small)\n  \u2514\u2500 Environment: QA (Medium)\n  \u2514\u2500 Environment: Prod (Large)\n       \u251c\u2500 Project: Customer-Onboarding\n       \u2514\u2500 Project: Legal-Document-Analysis\n</code></pre> <p>Creating a workspace \u2014 step by step:</p> <ul> <li> Go to Infra Hub \u2192 click the Workspaces tab \u2192 click <code>+ Workspace</code></li> <li> Choose your cloud provider: Microsoft Azure (most common), AWS, GCP, or NVIDIA NGC</li> <li> Enter the admin email \u2192 click Continue (triggers an approval email)</li> <li> Fill in workspace name, description, Azure subscription account, and region</li> <li> Add at least one environment. Click Add \u2192 choose type and size (start with Dev/Small)</li> <li> Click Submit \u2192 Proceed</li> </ul> <p>Choosing an environment size:</p> Size Use Case Small Exploration and early dev. Single small VM or low-end Kubernetes pod. Cheapest option. Medium QA and staging. Mirrors production performance without full scale. Large Production. High-throughput clusters, HA setup, disaster recovery. <p>Key insight</p> <p>Creating an environment only sets a quota \u2014 no real compute is reserved until a workload runs. You only pay for what you use.</p>"},{"location":"first-steps/#step-4-create-a-project","title":"Step 4 \u2014 Create a Project","text":"<p>A project is scoped to a workspace and environment. Every pipeline you build, every AI service you deploy, and every data experiment you run belongs to a project.</p> <ul> <li> In Infra Hub, click the Projects tab \u2192 <code>+ Project</code></li> <li> Enter a project name (e.g., <code>customer-onboarding-v1</code>) and a short description (you can't rename projects later)</li> <li> Select the parent Workspace from the dropdown</li> <li> Assign team members: click <code>+ Assign</code> \u2192 search by name \u2192 select role (Platform Admin, AI Engineer, QA, Architect, Auditor)</li> <li> Select the environment(s) to attach (start with Dev)</li> <li> Click Submit \u2192 OK</li> </ul> <p>Switching into a project: All work in Config view happens within a project context.</p> <ol> <li>Click the role/view dropdown (top-left)</li> <li>Select Config to enter Config View</li> <li>Click the project-switcher icon (\u2194) next to the project name</li> <li>Select your project and environment \u2192 click Submit</li> </ol> <p>Tip</p> <p>Always verify which project and environment you're in before deploying a service or running a pipeline. Deploying to production when you meant to use Dev is a common mistake.</p>"},{"location":"first-steps/#step-5-deploy-an-ai-service","title":"Step 5 \u2014 Deploy an AI Service","text":"<p>An AI service is the unit of intelligence on AIDF \u2014 every pipeline node is an AI service.</p> <p>Info</p> <p>If you just want to use existing services, you can build a pipeline using services that are already deployed and public in your workspace. This step is for AI Engineers building a new service.</p>"},{"location":"first-steps/#option-a-use-an-existing-public-service","title":"Option A \u2014 Use an existing public service","text":"<ul> <li> Switch to Config View in Infra Hub \u2192 click AI Services</li> <li> Use the Components dropdown to browse by category (Ingestion, Pre-processing, Storage, etc.)</li> <li> Click Action \u2192 Deploy on any public service you want to use in your project</li> </ul>"},{"location":"first-steps/#option-b-deploy-a-new-service-youve-built","title":"Option B \u2014 Deploy a new service you've built","text":"<ul> <li> Zip your entire service directory: <code>zip -r my_service.zip my_service/</code> (ZIP name must match the <code>&lt;Name&gt;</code> field in manifest.xml)</li> <li> Upload the ZIP to the Public Artifactory using the Agile Workbench VS Code plugin</li> <li> In Infra Hub Config View \u2192 AI Services \u2192 click <code>+ Add</code></li> <li> Select visibility: Private (this project only) or Public (all projects in workspace)</li> <li> Select your service ZIP from the list \u2192 click Submit (Manifest Engine builds image \u2192 pushes to ACR \u2192 deploys to AKS)</li> <li> Click Action \u2192 Test with a sample payload to verify output</li> <li> A Platform Admin reviews and approves your service in My Reviews</li> </ul> <p>Tip</p> <p>While you wait for approval, write your <code>auto_qa</code> unit tests and verify <code>input.json</code> schema matches your service's actual payload.</p>"},{"location":"first-steps/#step-6-build-your-first-pipeline","title":"Step 6 \u2014 Build Your First Pipeline","text":"<p>A pipeline template is an ordered set of AI service nodes. Once built and approved, a template becomes a live instance in GenAI Studio.</p> <ul> <li> In Infra Hub Config View \u2192 click Pipeline Template \u2192 <code>+ Pipeline</code></li> <li> Browse services by component. Drag your first service onto the canvas</li> <li> Draw a line from the Start node to your first service box</li> <li> Add more services and connect them in sequence</li> <li> Configure each node: double-click a service box \u2192 fill in parameters \u2192 click Set</li> <li> (Optional) Add conditional branching: double-click an arrow \u2192 write the condition using output KPI variables (e.g., <code>pii_detected == true</code>)</li> <li> Enter a Pipeline Name, description, and category \u2192 click Create</li> </ul> <p>Minimal example \u2014 Video object-detection pipeline:</p> <pre><code>Start \u2192 Video Ingestion (Azure Blob \u2192 AIDF) \u2192 YOLO Detection \u2192 Output to Data Hub\n</code></pre> <p>Getting the pipeline approved:</p> <ul> <li> Click Action \u2192 Test with a sample input. Check status: Initiated \u2192 Success</li> <li> Ask a Platform Admin to review it in My Reviews \u2192 approve</li> <li> After approval, your pipeline template becomes available as an instance in GenAI Studio</li> </ul>"},{"location":"first-steps/#step-7-run-inspect-results","title":"Step 7 \u2014 Run &amp; Inspect Results","text":""},{"location":"first-steps/#running-the-pipeline-in-genai-studio","title":"Running the pipeline in GenAI Studio","text":"<ul> <li> Navigate to GenAI Studio from the app dashboard</li> <li> Find your approved pipeline in the pipeline list</li> <li> Click the pipeline \u2192 click Run (or schedule it)</li> <li> Confirm the input payload \u2192 click Execute</li> <li> Click View Result once status shows Success (see per-service output, KPIs, execution time)</li> <li> Click View Execution Trajectory to trace data flow through each node</li> </ul>"},{"location":"first-steps/#verifying-output-in-data-hub","title":"Verifying output in Data Hub","text":"<p>If your pipeline registers output in Data Hub (<code>Storage Type: Physical</code>):</p> <ul> <li> Navigate to Data Hub \u2192 select your Industry section</li> <li> Find the dataset tile matching your pipeline output \u2192 click Get Details</li> <li> Click the Data tab \u2192 verify new files with expected format and IDs</li> <li> Check the Lineage tab to confirm the pipeline as the source</li> </ul>"},{"location":"first-steps/#troubleshooting","title":"Troubleshooting","text":"Symptom Action Pipeline Failed Click the failed node in the execution trajectory. Check Service Output for error message. Look at container console logs for stack trace. Service not available Go to Infra Hub \u2192 Config View \u2192 AI Services. Check the Deploy Status column \u2014 it may not be approved yet. Empty output Check that the source container name and folder path match the actual input data location in Azure Blob Storage. Wrong results Verify <code>config/input.json</code> payload. Run the service individually via HTTP endpoint using Postman before placing it in a pipeline."},{"location":"first-steps/#youre-up-and-running","title":"You're up and running!","text":"<p>Your first-steps checklist:</p> <ul> <li> I understand the six core AIDF modules and how data flows between them</li> <li> I can log in and navigate between Admin View and Config View in Infra Hub</li> <li> I know what a workspace is and how to create one (or find mine)</li> <li> I have been added to a project and can switch into it</li> <li> I can browse the AI services catalog and deploy/use existing services</li> <li> I can build a simple pipeline by dragging services onto the canvas and configuring each node</li> <li> I can run a pipeline, monitor its status, and inspect results in Data Hub</li> </ul> <p>Where to go next:</p> <ul> <li>Data Hub \u2192 \u2014 Ingest datasets, set up HIL annotation tasks, use synthetic data generation</li> <li>AI Workbench \u2192 \u2014 Benchmark models, fine-tune with your data, run governance assessments</li> <li>Agent Workbench \u2192 \u2014 Automate a business process end-to-end using LangGraph</li> <li>Developer Guide \u2192 \u2014 Write a new AI service from scratch</li> </ul> <p>Questions?</p> <p>Post them to the AIDF community forum (link shared by Ravi during training) \u2014 the team commits to responding by the next business day. For code-level questions, reach out to <code>ai-services-team@centific.com</code>.</p>"},{"location":"tutorial/","title":"Build an AI Service \u2014 Full Tutorial","text":"<p>A complete, code-first walkthrough building a Support Ticket Classifier AI Service on AIDF from scratch. The service uses Azure OpenAI GPT-4o to classify support tickets by category, urgency, sentiment, and routes them to the correct support queue.</p>"},{"location":"tutorial/#prerequisites","title":"Prerequisites","text":"Requirement Details AIDF Access AI Engineer role in at least one Workspace + Project with Dev environment set up Python 3.11 Exact version required \u2014 matches the Dockerfile base image Docker Desktop For local build and test before deploying via Infra Hub Azure CLI <code>az login</code> \u2014 needed to pull from the private Azure Artifacts feed Azure OpenAI access A GPT-4o deployment endpoint + API key (provided via Azure App Configuration by the platform admin) Reference Template Clone or copy the <code>reference_template</code> project from the Code directory VS Code Recommended IDE; Python and Docker extensions helpful <p>Tip</p> <p>You don't need to set up Azure OpenAI yourself. The platform admin configures all credentials in Azure App Configuration. Your service fetches them automatically via the <code>ConfigLoader</code> \u2014 you just reference the key name.</p>"},{"location":"tutorial/#architecture-overview","title":"Architecture Overview","text":"<p>Every AI Service on AIDF follows the Strategy Design Pattern. There are three layers:</p> <pre><code>Abstract_Class.py  \u2192  aiservice.py (you write this)  \u2192  Infra Hub / AKS\n(Platform)            (Your logic)                      (Platform)\n</code></pre> Layer File Who Owns It What It Does Abstract Contract <code>Abstract_Class.py</code> Platform (don't modify) Defines ~20 abstract methods every AI Service must implement. Enforces the interface. Concrete Implementation <code>aiservice.py</code> You Your business logic \u2014 the actual LLM calls, data processing, ML inference. Implements all abstract methods. Config System <code>environment.py</code> You (thin wrapper) Pulls all credentials and settings from Azure App Configuration. Singleton pattern. Deployment Blueprint <code>manifest.xml</code> You Tells the Manifest Engine how to build, push, and deploy your container. Container <code>Dockerfile</code> You (template) Packages your Flask service into a Docker image for AKS deployment. <p>Request Lifecycle: When a client POSTs a ticket to your service:</p> <ol> <li>Flask route receives the JSON body</li> <li><code>ModelStrategy()</code> is instantiated \u2014 fresh per request</li> <li><code>on_start()</code> \u2014 initializes timing and stores raw input</li> <li><code>execute()</code> \u2014 validates, preprocesses, calls GPT-4o, returns raw result</li> <li><code>on_finish()</code> \u2014 wraps result in the standard AIDF response envelope</li> <li><code>monitor_performance()</code> and <code>explainability_monitor()</code> \u2014 emit telemetry to ELK</li> <li>Flask returns <code>200 + JSON</code> to the caller</li> </ol> <p>If any step throws an exception, <code>on_error()</code> catches it and returns a structured error response.</p>"},{"location":"tutorial/#project-structure","title":"Project Structure","text":"<p>Start by copying the <code>reference_template</code> directory and renaming it:</p> <pre><code>ticket-classifier/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 Abstract_Class.py        # Platform contract \u2014 do NOT modify\n\u2502   \u251c\u2500\u2500 aiservice.py             # \u2190 YOUR main implementation file\n\u2502   \u251c\u2500\u2500 environment.py           # Config loader (thin wrapper)\n\u2502   \u251c\u2500\u2500 constants.py             # Enums: categories, urgency, queues\n\u2502   \u2514\u2500\u2500 api_data_access_layer.py # PostgreSQL logging helpers\n\u251c\u2500\u2500 scripts/\n\u2502   \u2514\u2500\u2500 manifest.xml             # Deployment blueprint for Manifest Engine\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 input.json               # Input payload schema\n\u2502   \u251c\u2500\u2500 output.json              # Expected output shape\n\u2502   \u251c\u2500\u2500 feedback.json            # HIL feedback schema\n\u2502   \u2514\u2500\u2500 env.config               # Environment variables (not committed to git)\n\u251c\u2500\u2500 lib/\n\u2502   \u2514\u2500\u2500 requirements.txt         # Python dependencies\n\u251c\u2500\u2500 auto_qa/\n\u2502   \u2514\u2500\u2500 ticket_classifier_unit_test.py\n\u2514\u2500\u2500 Dockerfile\n</code></pre> <p>Convention</p> <p>The platform's Manifest Engine expects this exact folder layout. Do not rename <code>src/</code>, <code>scripts/</code>, <code>config/</code>, or <code>lib/</code>.</p>"},{"location":"tutorial/#step-1-the-abstract-contract","title":"Step 1 \u2014 The Abstract Contract","text":"<p>This file comes from the platform. You never modify it. It defines the interface your service must implement.</p> src/Abstract_Class.py<pre><code>from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Optional\n\n\nclass GENAI_Abstract_Class(ABC):\n    \"\"\"\n    Platform-enforced base class for all AIDF AI Services.\n    Every concrete implementation must override all abstract methods.\n    \"\"\"\n\n    # \u2500\u2500\u2500 Lifecycle Methods \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n    @abstractmethod\n    def on_start(self, input_data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Called at the very beginning of each request.\n        Initialize timing, set instance state, return init status.\"\"\"\n        pass\n\n    @abstractmethod\n    def validate_input_data(self) -&gt; bool:\n        \"\"\"Validate that all required fields are present and well-formed.\n        Raise ValueError with a descriptive message on failure.\"\"\"\n        pass\n\n    @abstractmethod\n    def preprocess_input_data(self) -&gt; Dict[str, Any]:\n        \"\"\"Extract, clean, and normalize the raw input payload.\"\"\"\n        pass\n\n    @abstractmethod\n    def execute(self, input_data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Core business logic: LLM calls, ML inference, data processing.\"\"\"\n        pass\n\n    @abstractmethod\n    def on_finish(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Wrap result in the standard AIDF response envelope.\"\"\"\n        pass\n\n    @abstractmethod\n    def on_error(self, error_message: str, error_code: int) -&gt; Dict[str, Any]:\n        \"\"\"Return a structured error response.\"\"\"\n        pass\n\n    # \u2500\u2500\u2500 Monitoring &amp; Observability \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n    @abstractmethod\n    def monitor_performance(self, execution_time: float) -&gt; None: pass\n\n    @abstractmethod\n    def explainability_monitor(self, input_data: Dict, output_data: Dict) -&gt; None: pass\n\n    @abstractmethod\n    def alert_system(self, message: str) -&gt; None: pass\n\n    @abstractmethod\n    def model_drift_check(self) -&gt; None: pass\n\n    @abstractmethod\n    def data_quality_check(self, data: Any) -&gt; bool: pass\n\n    # \u2500\u2500\u2500 Compute &amp; Caching \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n    @abstractmethod\n    def compute_offload(self, payload: Any) -&gt; Any: pass\n\n    @abstractmethod\n    def cache_check(self, key: str) -&gt; Optional[Any]: pass\n\n    @abstractmethod\n    def cache_store(self, key: str, value: Any) -&gt; None: pass\n\n    # \u2500\u2500\u2500 Platform Hooks \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n    @abstractmethod\n    def pre_process_hook(self) -&gt; None: pass\n\n    @abstractmethod\n    def post_process_hook(self) -&gt; None: pass\n\n    @abstractmethod\n    def get_model_metadata(self) -&gt; Dict[str, Any]: pass\n\n    @abstractmethod\n    def health_check(self) -&gt; Dict[str, Any]: pass\n</code></pre> <p>Why so many methods?</p> <p>The platform enforces this interface so every AI Service is observable, auditable, and manageable in a uniform way \u2014 regardless of whether it's a computer vision model, an LLM chain, or a classical ML pipeline.</p>"},{"location":"tutorial/#step-2-config-environment","title":"Step 2 \u2014 Config &amp; Environment","text":"<p>The platform stores all credentials in Azure App Configuration. Your service fetches them at startup via the <code>ConfigLoader</code> singleton. You never hardcode secrets.</p> src/environment.py<pre><code>import os\nfrom dataclasses import dataclass\nfrom ai_service_common_utils.app_configuration import get_config_keys\n\nENV = os.getenv(\"ENVIRONMENT\", \"Development\")\n\n\nclass ConfigLoader:\n    \"\"\"Singleton that connects to Azure App Configuration once at startup.\"\"\"\n    _instance = None\n\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n            cls._instance._initialize_azure_app_config()\n        return cls._instance\n\n    def _initialize_azure_app_config(self):\n        trim_prefixes = [\n            \"Common.Be.\",\n            \"PaasInfra.Be.\",\n            \"DataMarketPlace.Be.\",\n            \"Datahub.Be.\",\n            \"Modalfoundry.Be.\",\n        ]\n        self.app_config_client = get_config_keys(trim_prefixes)\n\n    def get(self, key: str, default=None):\n        try:\n            return self.app_config_client.get(key, default)\n        except Exception:\n            return os.getenv(key, default)\n\n\nconfig_loader = ConfigLoader()\n\n\n@dataclass\nclass Config:\n    # \u2500\u2500 Azure OpenAI \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    AZURE_OPENAI_ENDPOINT:    str = config_loader.get(\"AZURE_OPENAI_ENDPOINT\")\n    AZURE_OPENAI_KEY:         str = config_loader.get(\"AZURE_OPENAI_KEY\")\n    AZURE_OPENAI_DEPLOYMENT:  str = config_loader.get(\"AZURE_OPENAI_DEPLOYMENT\", \"gpt-4o\")\n    AZURE_OPENAI_API_VERSION: str = config_loader.get(\"AZURE_OPENAI_API_VERSION\", \"2024-08-01-preview\")\n\n    # \u2500\u2500 PostgreSQL \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    DB_CONNECTION_STRING: str = config_loader.get(\"DB_CONNECTION_STRING\")\n    DB_SCHEMA:            str = config_loader.get(\"DB_SCHEMA\", \"aiservices\")\n\n    # \u2500\u2500 Service Identity \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    AI_SERVICE_ID: str = config_loader.get(\"AI_SERVICE_ID\", \"ticket-classifier\")\n    WORKSPACE_ID:  str = config_loader.get(\"WORKSPACE_ID\")\n    PROJECT_ID:    str = config_loader.get(\"PROJECT_ID\")\n</code></pre> config/env.config  \u26a0 Add to .gitignore<pre><code>AZURE_APP_CONFIG_CONNECTION_STRING=Endpoint=https://your-appconfig.azconfig.io;Id=XXXX;Secret=XXXX\nAZURE_OPENAI_ENDPOINT=https://your-oai-resource.openai.azure.com/\nAZURE_OPENAI_KEY=your-api-key-here\nAZURE_OPENAI_DEPLOYMENT=gpt-4o\nAZURE_OPENAI_API_VERSION=2024-08-01-preview\nDB_CONNECTION_STRING=postgresql://user:password@host:5432/aiservices_db\nAI_SERVICE_ID=ticket-classifier\nENVIRONMENT=Development\nFLASK_APP=src/aiservice.py\nFLASK_RUN_HOST=0.0.0.0\nFLASK_RUN_PORT=8002\n</code></pre> <p>Local Dev</p> <p>Load env.config before starting: <code>export $(grep -v '^#' config/env.config | xargs) &amp;&amp; flask run</code></p>"},{"location":"tutorial/#step-3-constants","title":"Step 3 \u2014 Constants","text":"<p>Define all magic strings as <code>Enum</code> classes to prevent typos and get IDE autocomplete.</p> src/constants.py<pre><code>from enum import Enum\n\n\nclass TICKET_CLASSIFIER(Enum):\n    MODULE       = \"ticket_commandpattern\"\n    SERVICE_NAME = \"support_ticket_classifier\"\n    VERSION      = \"1.0.0\"\n\n\nclass TICKET_CATEGORY(Enum):\n    BILLING   = \"Billing\"\n    TECHNICAL = \"Technical\"\n    ACCOUNT   = \"Account\"\n    SHIPPING  = \"Shipping\"\n    PRODUCT   = \"Product\"\n    GENERAL   = \"General\"\n\n\nclass TICKET_URGENCY(Enum):\n    LOW      = \"Low\"\n    MEDIUM   = \"Medium\"\n    HIGH     = \"High\"\n    CRITICAL = \"Critical\"\n\n\nclass TICKET_SENTIMENT(Enum):\n    POSITIVE   = \"Positive\"\n    NEUTRAL    = \"Neutral\"\n    FRUSTRATED = \"Frustrated\"\n    ANGRY      = \"Angry\"\n\n\nclass ROUTING_QUEUE(Enum):\n    L1_GENERAL     = \"L1-General\"\n    L1_BILLING     = \"L1-Billing\"\n    L2_TECHNICAL   = \"L2-Technical\"\n    L2_ACCOUNT     = \"L2-Account\"\n    L3_ESCALATION  = \"L3-Escalation\"\n    PRIORITY_QUEUE = \"Priority-Queue\"\n\n\nclass COMMAND(Enum):\n    INFERENCE = \"INFERENCE\"\n    BATCH     = \"BATCH\"\n    HEALTH    = \"HEALTH\"\n\n\nclass ELK_INDEX(Enum):\n    ELK_INDEX_NAME    = \"datafactory_logs\"\n    SERVICE_LOG_INDEX = \"ticket_classifier_logs\"\n</code></pre>"},{"location":"tutorial/#step-4-data-access-layer","title":"Step 4 \u2014 Data Access Layer","text":"<p>Handles all database interactions \u2014 logging every classification result to PostgreSQL for HIL review, analytics, and drift detection.</p> src/api_data_access_layer.py<pre><code>import psycopg2\nfrom environment import Config\n\n\ndef postgres_connection(env: str = \"stage\"):\n    connection = psycopg2.connect(\n        Config.DB_CONNECTION_STRING,\n        options=f\"-c search_path={Config.DB_SCHEMA}\"\n    )\n    return connection\n\n\ndef log_classification_result(\n    ticket_id: str, category: str, urgency: str,\n    sentiment: str, confidence: float, execution_time: float\n) -&gt; None:\n    conn = None\n    try:\n        conn = postgres_connection()\n        cursor = conn.cursor()\n        cursor.execute(\"\"\"\n            INSERT INTO ticket_classifications\n                (ticket_id, category, urgency, sentiment,\n                 confidence_score, execution_time_ms, created_at)\n            VALUES (%s, %s, %s, %s, %s, %s, NOW())\n            ON CONFLICT (ticket_id) DO UPDATE SET\n                category         = EXCLUDED.category,\n                urgency          = EXCLUDED.urgency,\n                sentiment        = EXCLUDED.sentiment,\n                confidence_score = EXCLUDED.confidence_score,\n                updated_at       = NOW()\n        \"\"\", (ticket_id, category, urgency, sentiment,\n              round(confidence, 4), round(execution_time * 1000, 2)))\n        conn.commit()\n    except Exception as e:\n        if conn:\n            conn.rollback()\n        raise e\n    finally:\n        if conn:\n            conn.close()\n</code></pre> <p>DB Schema</p> <p>The <code>ticket_classifications</code> table lives in your service's PostgreSQL schema, provisioned by the platform when you create the environment. The <code>DBService</code> entry in <code>manifest.xml</code> (Step 9) tells the platform to provision this schema automatically.</p>"},{"location":"tutorial/#step-5-core-service-logic-aiservicepy","title":"Step 5 \u2014 Core Service Logic (<code>aiservice.py</code>)","text":"<p>This is the main file you write. It contains your <code>ModelStrategy</code> class and Flask route definitions.</p>"},{"location":"tutorial/#imports-setup","title":"Imports &amp; Setup","text":"src/aiservice.py \u2014 Imports &amp; Setup<pre><code>import time, json, hashlib, logging\nfrom typing import Dict, Any, Optional\n\nfrom flask import Flask, request, jsonify\nfrom flask_cors import CORS\nfrom openai import AzureOpenAI\n\nfrom Abstract_Class import GENAI_Abstract_Class\nfrom environment import Config\nfrom constants import (\n    TICKET_CLASSIFIER, TICKET_CATEGORY, TICKET_URGENCY,\n    TICKET_SENTIMENT, ROUTING_QUEUE, COMMAND, ELK_INDEX\n)\nfrom api_data_access_layer import log_classification_result\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')\nlogger = logging.getLogger(__name__)\n\napp = Flask(__name__)\nCORS(app)  # Required: pipeline canvas calls services cross-origin\n\nopenai_client = AzureOpenAI(\n    azure_endpoint=Config.AZURE_OPENAI_ENDPOINT,\n    api_key=Config.AZURE_OPENAI_KEY,\n    api_version=Config.AZURE_OPENAI_API_VERSION,\n)\n\nROUTING_MAP = {\n    (\"Billing\",   \"Low\"):      ROUTING_QUEUE.L1_BILLING.value,\n    (\"Billing\",   \"High\"):     ROUTING_QUEUE.L3_ESCALATION.value,\n    (\"Billing\",   \"Critical\"): ROUTING_QUEUE.PRIORITY_QUEUE.value,\n    (\"Technical\", \"Low\"):      ROUTING_QUEUE.L2_TECHNICAL.value,\n    (\"Technical\", \"High\"):     ROUTING_QUEUE.L3_ESCALATION.value,\n    (\"Technical\", \"Critical\"): ROUTING_QUEUE.PRIORITY_QUEUE.value,\n    (\"Account\",   \"Low\"):      ROUTING_QUEUE.L2_ACCOUNT.value,\n    (\"Account\",   \"Critical\"): ROUTING_QUEUE.PRIORITY_QUEUE.value,\n    # ... (full map covers all category \u00d7 urgency combinations)\n}\n</code></pre>"},{"location":"tutorial/#on_start-initialize-request-context","title":"<code>on_start()</code> \u2014 Initialize Request Context","text":"src/aiservice.py \u2014 on_start()<pre><code>class ModelStrategy(GENAI_Abstract_Class):\n\n    def __init__(self):\n        self.start_time:      float = 0.0\n        self.input_data:      Dict[str, Any] = {}\n        self.processed_input: Dict[str, Any] = {}\n        self.module_name:     str = TICKET_CLASSIFIER.MODULE.value\n\n    def on_start(self, input_data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        self.start_time = time.time()\n        self.input_data = input_data\n        ticket_id = input_data.get(\"input_payload\", {}).get(\"ticket_id\", {}).get(\"value\", \"unknown\")\n        logger.info(f\"[on_start] Request received | ticket_id={ticket_id}\")\n        return {\"status\": \"initialized\", \"ticket_id\": ticket_id}\n</code></pre>"},{"location":"tutorial/#validate_input_data-input-validation","title":"<code>validate_input_data()</code> \u2014 Input Validation","text":"src/aiservice.py \u2014 validate_input_data()<pre><code>    def validate_input_data(self) -&gt; bool:\n        payload = self.input_data.get(\"input_payload\", {})\n\n        command = payload.get(\"command\", {}).get(\"value\", \"\")\n        if not command:\n            raise ValueError(\"Missing required field: input_payload.command.value\")\n        valid_commands = [c.value for c in COMMAND]\n        if command not in valid_commands:\n            raise ValueError(f\"Invalid command '{command}'. Must be one of: {valid_commands}\")\n\n        ticket_text = payload.get(\"ticket_text\", {}).get(\"value\", \"\")\n        if not ticket_text or not ticket_text.strip():\n            raise ValueError(\"Missing required field: input_payload.ticket_text.value cannot be empty\")\n        if len(ticket_text.strip()) &lt; 10:\n            raise ValueError(\"ticket_text must be at least 10 characters\")\n        if len(ticket_text) &gt; 8000:\n            raise ValueError(\"ticket_text exceeds maximum length of 8000 characters\")\n\n        return True\n</code></pre>"},{"location":"tutorial/#execute-core-inference","title":"<code>execute()</code> \u2014 Core Inference","text":"src/aiservice.py \u2014 execute()<pre><code>    def execute(self, input_data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        # Step 1: Validate &amp; Preprocess\n        self.validate_input_data()\n        preprocessed  = self.preprocess_input_data()\n        ticket_text   = preprocessed[\"ticket_text\"]\n        ticket_id     = preprocessed[\"ticket_id\"]\n        customer_tier = preprocessed[\"customer_tier\"]\n\n        # Step 2: Cache Check\n        cache_key = hashlib.md5(ticket_text.encode()).hexdigest()\n        cached = self.cache_check(cache_key)\n        if cached:\n            return cached\n\n        # Step 3: Build Prompt\n        tier_note = \"\"\n        if customer_tier in (\"premium\", \"enterprise\"):\n            tier_note = \"\\nIMPORTANT: Automatically elevate urgency by one step.\"\n\n        system_prompt = \"\"\"You are an expert customer support triage specialist.\nAnalyze the given support ticket and return a JSON object with EXACTLY these fields:\n{\n  \"category\":    one of [\"Billing\", \"Technical\", \"Account\", \"Shipping\", \"Product\", \"General\"],\n  \"subcategory\": a brief phrase (max 5 words),\n  \"urgency\":     one of [\"Low\", \"Medium\", \"High\", \"Critical\"],\n  \"sentiment\":   one of [\"Positive\", \"Neutral\", \"Frustrated\", \"Angry\"],\n  \"confidence_score\": float between 0.0 and 1.0,\n  \"reasoning\":   one sentence explaining the classification,\n  \"auto_response_template\": a 2-3 sentence empathetic opening response\n}\nReturn ONLY the JSON object. No markdown fences.\"\"\"\n\n        user_prompt = f\"Support Ticket (ID: {ticket_id}):\\n---\\n{ticket_text}\\n---{tier_note}\\n\\nClassify this ticket now.\"\n\n        # Step 4: Call Azure OpenAI\n        response = openai_client.chat.completions.create(\n            model=Config.AZURE_OPENAI_DEPLOYMENT,\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\",   \"content\": user_prompt},\n            ],\n            temperature=0.1,\n            max_tokens=600,\n            response_format={\"type\": \"json_object\"},\n        )\n\n        # Step 5: Parse Response\n        classification = json.loads(response.choices[0].message.content)\n        cat  = classification.get(\"category\",  \"General\")\n        urg  = classification.get(\"urgency\",   \"Low\")\n        conf = float(classification.get(\"confidence_score\", 0.0))\n\n        # Step 6: Determine Routing Queue\n        recommended_queue = ROUTING_MAP.get((cat, urg), ROUTING_QUEUE.L1_GENERAL.value)\n\n        # Step 7: Persist to DB (non-fatal)\n        try:\n            log_classification_result(\n                ticket_id=ticket_id, category=cat, urgency=urg,\n                sentiment=classification.get(\"sentiment\", \"Neutral\"),\n                confidence=conf, execution_time=time.time() - self.start_time,\n            )\n        except Exception as db_err:\n            logger.warning(f\"[execute] DB log failed (non-fatal): {db_err}\")\n\n        result = {\n            \"ticket_id\":              ticket_id,\n            \"category\":               cat,\n            \"subcategory\":            classification.get(\"subcategory\", \"\"),\n            \"urgency\":                urg,\n            \"sentiment\":              classification.get(\"sentiment\", \"Neutral\"),\n            \"confidence_score\":       round(conf, 4),\n            \"reasoning\":              classification.get(\"reasoning\", \"\"),\n            \"recommended_queue\":      recommended_queue,\n            \"auto_response_template\": classification.get(\"auto_response_template\", \"\"),\n            \"tokens_used\":            response.usage.total_tokens,\n            \"customer_tier\":          customer_tier,\n        }\n\n        if conf &gt;= 0.85:\n            self.cache_store(cache_key, result)\n\n        return result\n</code></pre>"},{"location":"tutorial/#on_finish-on_error-response-envelope","title":"<code>on_finish()</code> &amp; <code>on_error()</code> \u2014 Response Envelope","text":"src/aiservice.py \u2014 on_finish() &amp; on_error()<pre><code>    def on_finish(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        execution_time = round(time.time() - self.start_time, 4)\n        return {\n            \"execution_status\": \"success\",\n            \"status_code\":      200,\n            \"output_kpis\": {\n                \"execution_time\":    execution_time,\n                \"processed_records\": 1,\n                \"tokens_used\":       data.get(\"tokens_used\", 0),\n            },\n            \"service_output\": {k: v for k, v in data.items() if k != \"tokens_used\"},\n        }\n\n    def on_error(self, error_message: str, error_code: int = 500) -&gt; Dict[str, Any]:\n        execution_time = round(time.time() - self.start_time, 4)\n        logger.error(f\"[on_error] code={error_code} | {error_message}\")\n        return {\n            \"execution_status\": \"failed\",\n            \"status_code\":      error_code,\n            \"error_message\":    str(error_message),\n            \"output_kpis\":      {\"execution_time\": execution_time},\n        }\n</code></pre>"},{"location":"tutorial/#flask-routes","title":"Flask Routes","text":"src/aiservice.py \u2014 Flask Routes<pre><code>@app.route('/api/&lt;ai_service_id&gt;/&lt;deploy_env_id&gt;/Basic_functionality', methods=['POST'])\ndef process_request(ai_service_id: str, deploy_env_id: str):\n    Model = ModelStrategy()\n    try:\n        input_data = request.get_json(force=True)\n        if not input_data:\n            return jsonify(Model.on_error(\"Empty or invalid JSON body\", 400)), 400\n\n        Model.on_start(input_data)\n        result   = Model.execute(input_data)\n        response = Model.on_finish(result)\n\n        Model.monitor_performance(response[\"output_kpis\"][\"execution_time\"])\n        Model.explainability_monitor(input_data, response)\n\n        return jsonify(response), 200\n\n    except ValueError as ve:\n        return jsonify(Model.on_error(str(ve), error_code=422)), 422\n    except Exception as e:\n        logger.exception(f\"[process_request] Unhandled exception: {e}\")\n        return jsonify(Model.on_error(str(e), error_code=500)), 500\n\n\n@app.route('/health', methods=['GET'])\ndef health():\n    return jsonify(ModelStrategy().health_check()), 200\n\n\n@app.route('/metadata', methods=['GET'])\ndef metadata():\n    return jsonify(ModelStrategy().get_model_metadata()), 200\n\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=8002, debug=False)\n</code></pre>"},{"location":"tutorial/#step-6-input-output-schemas","title":"Step 6 \u2014 Input / Output Schemas","text":"config/input.json<pre><code>{\n  \"input_payload\": {\n    \"command\": {\n      \"value\": \"INFERENCE\",\n      \"data_type\": \"string\",\n      \"is_mandatory\": \"Y\",\n      \"default_value\": \"INFERENCE\",\n      \"source_from\": \"runtime\",\n      \"description\": \"Processing mode. INFERENCE=single ticket, BATCH=bulk, HEALTH=probe.\"\n    },\n    \"ticket_text\": {\n      \"value\": \"\",\n      \"data_type\": \"string\",\n      \"is_mandatory\": \"Y\",\n      \"default_value\": \"\",\n      \"source_from\": \"runtime\",\n      \"description\": \"The raw text of the support ticket. Min 10 chars, max 8000 chars.\"\n    },\n    \"ticket_id\": {\n      \"value\": \"\",\n      \"data_type\": \"string\",\n      \"is_mandatory\": \"N\",\n      \"source_from\": \"runtime\",\n      \"description\": \"Optional unique identifier. Auto-generated if not provided.\"\n    },\n    \"customer_tier\": {\n      \"value\": \"standard\",\n      \"data_type\": \"string\",\n      \"is_mandatory\": \"N\",\n      \"source_from\": \"runtime\",\n      \"description\": \"standard | premium | enterprise. Premium/enterprise escalates urgency.\"\n    }\n  }\n}\n</code></pre> config/output.json<pre><code>{\n  \"execution_status\": \"success\",\n  \"status_code\": 200,\n  \"output_kpis\": {\n    \"execution_time\": 1.243,\n    \"processed_records\": 1,\n    \"tokens_used\": 312\n  },\n  \"service_output\": {\n    \"ticket_id\": \"TK-1738291200\",\n    \"category\": \"Technical\",\n    \"subcategory\": \"Login failure 2FA\",\n    \"urgency\": \"High\",\n    \"sentiment\": \"Frustrated\",\n    \"confidence_score\": 0.94,\n    \"reasoning\": \"User cannot access account due to 2FA failure \u2014 High urgency.\",\n    \"recommended_queue\": \"L3-Escalation\",\n    \"auto_response_template\": \"Thank you for reaching out about this login issue...\",\n    \"customer_tier\": \"standard\"\n  }\n}\n</code></pre>"},{"location":"tutorial/#step-7-dockerfile","title":"Step 7 \u2014 Dockerfile","text":"Dockerfile<pre><code>FROM python:3.11\nWORKDIR /code\n\nCOPY lib/requirements.txt /code/requirements.txt\n\nARG ARTIFACTS_PAT\nRUN pip install --no-cache-dir --upgrade \\\n    -r requirements.txt \\\n    --extra-index-url \"https://build:${ARTIFACTS_PAT}@pkgs.dev.azure.com/YourOrg/YourProject/_packaging/YourFeed/pypi/simple/\"\n\nCOPY ./ /code/\n\nEXPOSE 8002\n\nENV FLASK_APP=src/aiservice.py \\\n    FLASK_RUN_HOST=0.0.0.0 \\\n    FLASK_RUN_PORT=8002 \\\n    PYTHONPATH=/code/src \\\n    ENVIRONMENT=Production\n\nCMD [\"flask\", \"run\"]\n</code></pre> <p>Security Note</p> <p>Never bake the <code>ARTIFACTS_PAT</code> or any secret directly into the Dockerfile. The platform's CI/CD pipeline injects it from Azure Key Vault.</p>"},{"location":"tutorial/#step-8-python-dependencies","title":"Step 8 \u2014 Python Dependencies","text":"lib/requirements.txt<pre><code># Platform packages (from private Azure Artifacts Feed)\nai-service-common-utils\nloop-common-utils==1.1203.0\n\n# Web Framework\nFlask==3.0.3\nFlask-Cors==4.0.1\n\n# Azure OpenAI\nopenai==1.40.0\n\n# Database\npsycopg2-binary==2.9.9\n\n# Utilities\npython-dotenv==1.0.1\n</code></pre> Package Purpose <code>ai-service-common-utils</code> Platform package \u2014 provides <code>GENAI_Abstract_Class</code>, ELK logger, platform monitoring hooks <code>loop-common-utils</code> Platform package \u2014 provides <code>get_config_keys()</code> for Azure App Configuration <code>Flask</code> Lightweight WSGI web framework for the REST API <code>Flask-Cors</code> CORS headers \u2014 required for the pipeline canvas to call your service from the browser <code>openai</code> Official Azure OpenAI Python SDK <code>psycopg2-binary</code> PostgreSQL database adapter <code>python-dotenv</code> Loads <code>env.config</code> for local development"},{"location":"tutorial/#step-9-deployment-blueprint-manifestxml","title":"Step 9 \u2014 Deployment Blueprint (manifest.xml)","text":"scripts/manifest.xml<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;Package&gt;\n\n  &lt;ArtifactInfo&gt;\n    &lt;Name&gt;ticket-classifier&lt;/Name&gt;\n    &lt;Module_Name&gt;LLM&lt;/Module_Name&gt;\n    &lt;Component&gt;Classification&lt;/Component&gt;\n    &lt;AIServiceVersion&gt;1&lt;/AIServiceVersion&gt;\n  &lt;/ArtifactInfo&gt;\n\n  &lt;Configuration&gt;\n\n    &lt;Deployment\n      type=\"Algo\"\n      technology=\"Azure\"\n      route=\"/Basic_functionality\"\n      cluster=\"shared\"&gt;\n\n      &lt;DevSize     DR=\"yes\"&gt;small&lt;/DevSize&gt;\n      &lt;QaSize      DR=\"yes\"&gt;medium&lt;/QaSize&gt;\n      &lt;StagingSize DR=\"yes\"&gt;medium&lt;/StagingSize&gt;\n      &lt;ProdSize    DR=\"yes\"&gt;large&lt;/ProdSize&gt;\n    &lt;/Deployment&gt;\n\n    &lt;DBServices&gt;\n      &lt;DBService type=\"SQL\" technology=\"AzureSQL\"&gt;\n        &lt;Schema&gt;aiservices&lt;/Schema&gt;\n        &lt;Table&gt;ticket_classifications&lt;/Table&gt;\n      &lt;/DBService&gt;\n    &lt;/DBServices&gt;\n\n    &lt;ContainerRegistry&gt;\n      &lt;Registry&gt;yourorg.azurecr.io&lt;/Registry&gt;\n      &lt;Repository&gt;ticket-classifier&lt;/Repository&gt;\n      &lt;BuildContext&gt;./&lt;/BuildContext&gt;\n      &lt;DockerfilePath&gt;./Dockerfile&lt;/DockerfilePath&gt;\n    &lt;/ContainerRegistry&gt;\n\n  &lt;/Configuration&gt;\n\n&lt;/Package&gt;\n</code></pre> Field What It Controls <code>&lt;Name&gt;</code> The <code>ai_service_id</code> used in the Flask URL path. Must match exactly. <code>&lt;Module_Name&gt;</code> How Infra Hub categorizes your service (LLM, CV, Classical ML, etc.) <code>route=\"/Basic_functionality\"</code> Suffix of your Flask endpoint URL after <code>/api/{id}/{env}/</code> <code>cluster=\"shared\"</code> Deploys to the shared AKS cluster. Use <code>\"dedicated\"</code> for high-throughput. <code>&lt;DBService&gt;</code> Tells the engine to provision a PostgreSQL schema + table at deployment time."},{"location":"tutorial/#step-10-unit-tests","title":"Step 10 \u2014 Unit Tests","text":"auto_qa/ticket_classifier_unit_test.py<pre><code>import os, json, unittest, requests\n\nBASE_URL      = os.getenv(\"SERVICE_BASE_URL\", \"http://localhost:8002\")\nAI_SERVICE_ID = os.getenv(\"AI_SERVICE_ID\",    \"ticket-classifier\")\nENV_ID        = os.getenv(\"DEPLOY_ENV_ID\",    \"dev\")\nENDPOINT      = f\"{BASE_URL}/api/{AI_SERVICE_ID}/{ENV_ID}/Basic_functionality\"\n\nCONFIG_DIR = os.path.join(os.path.dirname(__file__), \"..\", \"config\")\nwith open(os.path.join(CONFIG_DIR, \"input.json\"))  as f: INPUT_SCHEMA  = json.load(f)\nwith open(os.path.join(CONFIG_DIR, \"output.json\")) as f: OUTPUT_SCHEMA = json.load(f)\n\n\nclass TestTicketClassifier(unittest.TestCase):\n\n    def _call_service(self, ticket_text, ticket_id=\"TEST-001\", customer_tier=\"standard\"):\n        payload = {\n            \"input_payload\": {\n                \"command\":       {\"value\": \"INFERENCE\", \"data_type\": \"string\", \"is_mandatory\": \"Y\"},\n                \"ticket_text\":   {\"value\": ticket_text, \"data_type\": \"string\", \"is_mandatory\": \"Y\"},\n                \"ticket_id\":     {\"value\": ticket_id,   \"data_type\": \"string\", \"is_mandatory\": \"N\"},\n                \"customer_tier\": {\"value\": customer_tier, \"data_type\": \"string\", \"is_mandatory\": \"N\"},\n            }\n        }\n        return requests.post(ENDPOINT, json=payload, timeout=30)\n\n    def test_response_has_required_envelope_fields(self):\n        resp = self._call_service(\"I cannot log into my account. Please help urgently.\")\n        self.assertEqual(resp.status_code, 200)\n        body = resp.json()\n        self.assertIn(\"execution_status\", body)\n        self.assertIn(\"service_output\",   body)\n        self.assertEqual(body[\"execution_status\"], \"success\")\n\n    def test_billing_ticket_classified_correctly(self):\n        text = \"I was charged twice for my subscription this month. Please refund.\"\n        output = self._call_service(text, \"TEST-BILLING-001\").json()[\"service_output\"]\n        self.assertEqual(output[\"category\"], \"Billing\")\n        self.assertGreater(output[\"confidence_score\"], 0.7)\n\n    def test_empty_ticket_text_returns_422(self):\n        resp = self._call_service(\"\")\n        self.assertEqual(resp.status_code, 422)\n\n    def test_health_endpoint_returns_200(self):\n        resp = requests.get(f\"{BASE_URL}/health\", timeout=10)\n        self.assertEqual(resp.status_code, 200)\n        self.assertEqual(resp.json()[\"status\"], \"healthy\")\n\n\nif __name__ == \"__main__\":\n    unittest.main(verbosity=2)\n</code></pre>"},{"location":"tutorial/#step-11-local-testing","title":"Step 11 \u2014 Local Testing","text":"Flask (fast iteration)Docker (matches production) <pre><code># Load environment variables\nexport $(grep -v '^#' config/env.config | xargs)\n\n# Install dependencies\npython -m venv venv &amp;&amp; source venv/bin/activate\npip install -r lib/requirements.txt\n\n# Start the service\nflask run\n# \u2192 Running on http://0.0.0.0:8002\n</code></pre> <pre><code># Build\ndocker build --build-arg ARTIFACTS_PAT=\"${ARTIFACTS_PAT}\" -t ticket-classifier:local .\n\n# Run\ndocker run --rm --env-file config/env.config -p 8002:8002 ticket-classifier:local\n</code></pre> <p>Send a test request:</p> <pre><code>curl -s -X POST http://localhost:8002/api/ticket-classifier/dev/Basic_functionality \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"input_payload\": {\n      \"command\":       {\"value\": \"INFERENCE\", \"data_type\": \"string\", \"is_mandatory\": \"Y\"},\n      \"ticket_text\":   {\"value\": \"I was charged twice for my order last week and I need an immediate refund!\", \"data_type\": \"string\", \"is_mandatory\": \"Y\"},\n      \"ticket_id\":     {\"value\": \"TK-20260220-001\", \"data_type\": \"string\", \"is_mandatory\": \"N\"},\n      \"customer_tier\": {\"value\": \"standard\", \"data_type\": \"string\", \"is_mandatory\": \"N\"}\n    }\n  }' | python -m json.tool\n</code></pre> <p>Expected response:</p> <pre><code>{\n  \"execution_status\": \"success\",\n  \"status_code\": 200,\n  \"output_kpis\": { \"execution_time\": 1.47, \"processed_records\": 1 },\n  \"service_output\": {\n    \"ticket_id\": \"TK-20260220-001\",\n    \"category\": \"Billing\",\n    \"urgency\": \"High\",\n    \"sentiment\": \"Angry\",\n    \"confidence_score\": 0.96,\n    \"recommended_queue\": \"L3-Escalation\",\n    \"auto_response_template\": \"Thank you for bringing this billing issue to our attention...\"\n  }\n}\n</code></pre> <p>Run unit tests:</p> <pre><code>python -m pytest auto_qa/ticket_classifier_unit_test.py -v\n</code></pre>"},{"location":"tutorial/#step-12-deploy-via-infra-hub","title":"Step 12 \u2014 Deploy via Infra Hub","text":"<ol> <li>Navigate to Infra Hub \u2192 Your Project \u2192 AI Services</li> <li>Click \"Register AI Service\" \u2014 Name: <code>ticket-classifier</code>, Module: LLM, Version: 1</li> <li>Upload your code \u2014 push to the connected Git repository or use file upload</li> <li>Trigger the Manifest Engine \u2014 click \"Build &amp; Deploy\":<ul> <li>Reads <code>manifest.xml</code> to get build parameters</li> <li>Runs <code>docker build</code> with <code>ARTIFACTS_PAT</code> from Azure Key Vault</li> <li>Pushes image to ACR (<code>yourorg.azurecr.io/ticket-classifier:v1-dev</code>)</li> <li>Applies AKS deployment with <code>small</code> pod size (Dev)</li> <li>Provisions <code>ticket_classifications</code> table in PostgreSQL</li> </ul> </li> <li>Verify deployment \u2014 wait for all pods to show Running; platform calls <code>/health</code></li> <li>Run QA Tests from Infra Hub \u2014 Go to QA view \u2192 select your service \u2192 \"Run Auto QA\"</li> <li>Promote: Dev \u2192 QA \u2192 Staging \u2192 Production (each promotion re-deploys with appropriate pod size)</li> </ol> <p>After Deployment</p> <p>Your service is now available to the entire platform. The production URL will be: <code>https://aidf.yourorg.com/api/ticket-classifier/prod/Basic_functionality</code></p>"},{"location":"tutorial/#step-13-calling-the-deployed-service","title":"Step 13 \u2014 Calling the Deployed Service","text":"cURLPython <pre><code>curl -s -X POST \\\n  https://aidf.yourorg.com/api/ticket-classifier/prod/Basic_functionality \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer ${PLATFORM_TOKEN}\" \\\n  -d '{\n    \"input_payload\": {\n      \"command\":       {\"value\": \"INFERENCE\", \"data_type\": \"string\", \"is_mandatory\": \"Y\"},\n      \"ticket_text\":   {\"value\": \"My account was suspended without any warning.\", \"data_type\": \"string\", \"is_mandatory\": \"Y\"},\n      \"customer_tier\": {\"value\": \"enterprise\", \"data_type\": \"string\", \"is_mandatory\": \"N\"}\n    }\n  }'\n</code></pre> <pre><code>import requests\n\nresponse = requests.post(\n    \"https://aidf.yourorg.com/api/ticket-classifier/prod/Basic_functionality\",\n    headers={\"Authorization\": f\"Bearer {platform_token}\"},\n    json={\n        \"input_payload\": {\n            \"command\":     {\"value\": \"INFERENCE\", \"data_type\": \"string\", \"is_mandatory\": \"Y\"},\n            \"ticket_text\": {\"value\": \"My account was suspended.\", \"data_type\": \"string\", \"is_mandatory\": \"Y\"},\n        }\n    }\n)\nresult = response.json()\nprint(result[\"service_output\"][\"category\"])      # \"Account\"\nprint(result[\"service_output\"][\"urgency\"])        # \"High\"\nprint(result[\"service_output\"][\"recommended_queue\"])  # \"L3-Escalation\"\n</code></pre>"},{"location":"appendix/compute-offloading/","title":"Compute Offloading Architecture","text":"<p>For computationally intensive workloads (model training, large-scale inference), AIDF services offload execution to external GPU infrastructure while keeping the main service container lightweight.</p>"},{"location":"appendix/compute-offloading/#how-it-works","title":"How It Works","text":"<ol> <li>The AI service receives a training/inference request via its HTTP endpoint in AKS</li> <li>The <code>platform_cluster_manager.py</code> selects the appropriate compute provider based on configuration</li> <li>The selected deployment module (RunPod/Denvr/on-prem/Azure) provisions a GPU cluster or pod</li> <li>The Docker image (from ACR) is deployed to the GPU infrastructure</li> <li>The heavy computation runs on GPU hardware</li> <li>Results are stored in the central model repository (Azure Blob) or database</li> <li>The main service polls for completion and returns results to the caller</li> </ol>"},{"location":"appendix/compute-offloading/#supported-platforms","title":"Supported Platforms","text":"Platform Best For Billing RunPod Burst fine-tuning jobs Pay-per-use GPU pods Denvr Large-scale training with consistent performance High-performance dedicated GPU clusters On-Premises Data-sovereign workloads where data cannot leave the organization's network Internal GPU servers Azure Integrated Azure-native deployments AKS node pools with GPU-enabled VMs"},{"location":"appendix/environments/","title":"Deployment Environments","text":"Environment Purpose Compute Deployment Policy Development (Dev) Feature development, experiments, initial debugging Small VMs / low-end Kubernetes pods Rapid, unchecked deployments. Developers verify functionality. QA / Staging Systematic testing, integration checks, regression Medium instances, mirrors production scale Functional + regression + integration tests. Surfaces config issues before staging. Staging Near-production replica, load testing, user acceptance Medium-to-large, same images as Prod Isolated from live traffic. Performance benchmarking, security scans, stakeholder demos. Production (Prod) Live environment serving real users Large instances, HA clusters, optional multi-region Only thoroughly tested, approved versions deployed. Full DR and scaling policies."},{"location":"appendix/tech-stack/","title":"Technology Stack","text":"Layer Technology Purpose Front-End Angular Web application portal for browsing, configuring, and invoking AI services Back-End Python Flask Powers each AI service HTTP endpoint; orchestration logic; Azure component integration Data Storage PostgreSQL Primary relational database for metadata, execution logs, and application state Container Orchestration Azure Kubernetes Service (AKS) All AI services run in Docker containers, ensuring portability and scalability Container Registry Azure Container Registry (ACR) Stores private Docker images for all AI services Artifact Storage Azure Blob Storage Public Artifactory for service ZIPs; model artifacts; pipeline outputs Secret Management Azure Key Vault + HashiCorp Vault All connection strings, API keys, credentials stored and retrieved at runtime ML Experiment Tracking MLFlow Automatic experiment logging, comparison, and visualization for model training GPU Compute (Cloud) RunPod, Denvr, Azure GPU VMs On-demand GPU nodes for training and heavy inference Distributed Processing Ray Parallel task execution for large-scale video/data processing Logging ELK Stack (Elasticsearch) Structured execution logs indexed for search and monitoring IDE Integration Agile Workbench (VS Code plugin) Upload service ZIPs to Public Artifactory directly from VS Code"},{"location":"developer/","title":"What is an AI Service?","text":"<p>An AI Service in the AIDF ecosystem is a self-contained, deployable component that exposes a well-defined HTTP endpoint to perform one or more AI or data-processing tasks. It is the fundamental unit of all intelligence in the platform.</p>"},{"location":"developer/#core-properties","title":"Core Properties","text":"<ol> <li>Follows a Standard Folder Structure \u2014 ensures consistency across all services and teams</li> <li>Implements a Design-Pattern Backbone \u2014 Abstract Class + Concrete Class + Context/Strategy Class</li> <li>Exposes a Consistent Endpoint \u2014 pattern: <code>/datafactory/{module}/{serviceName}</code></li> </ol>"},{"location":"developer/#endpoint-pattern","title":"Endpoint Pattern","text":"<pre><code># Standard AIDF AI Service endpoint:\n/api/aiservice/centific/product/aidatafoundary/frontcontroller/\n  {workspace-name}/{project-name}/{serviceName}/{aiservice_id}/{deploy_env_id}/test\n</code></pre>"},{"location":"developer/#developer-guide-navigation","title":"Developer Guide Navigation","text":"<ul> <li>Folder Structure \u2014 Standard directory layout for every AI Service</li> <li>Strategy Design Pattern \u2014 Abstract Class \u2192 Concrete Class \u2192 Context</li> <li>Logging \u2014 Verbose and console log types</li> <li>Onboarding \u2014 Step-by-step deployment process</li> <li>Docker &amp; Deployment \u2014 Containerization and the Manifest Engine</li> <li>Manifest File \u2014 Deployment blueprint reference</li> </ul>"},{"location":"developer/docker/","title":"Docker &amp; Deployment","text":""},{"location":"developer/docker/#key-concepts","title":"Key Concepts","text":"<ul> <li>Image: A read-only template built from a Dockerfile. A snapshot of a filesystem plus metadata.</li> <li>Container: A running, read-write instance of an image. Start, stop, and delete without affecting the original image.</li> <li>ACR (Azure Container Registry): Microsoft's managed service for storing and managing private Docker images.</li> </ul>"},{"location":"developer/docker/#essential-docker-commands","title":"Essential Docker Commands","text":"<pre><code># Build image from Dockerfile in current directory\ndocker build -t myservice:latest .\n\n# Run container locally on port 8002\ndocker run -d -p 8002:8002 myservice:latest\n\n# Run with environment variable\ndocker run -d -p 8002:8002 -e ENVIRONMENT=Development myservice:latest\n\n# Push to Azure Container Registry\ndocker push myregistry.azurecr.io/myservice:latest\n\n# Pull from registry\ndocker pull myregistry.azurecr.io/myservice:latest\n</code></pre>"},{"location":"developer/docker/#standard-dockerfile-structure","title":"Standard Dockerfile Structure","text":"<pre><code>FROM python:3.11                          # Base Python 3.11 image\nWORKDIR /code                             # Set working directory\n\nCOPY lib/requirements.txt /code/requirements.txt\nRUN pip install --no-cache-dir -r requirements.txt \\\n    &amp;&amp; pip install internal-utils==x.y.z  # Internal utility library\n\nCOPY src/loop_log.ini /code/             # Copy logging config\nCOPY . /code                             # Copy all service files\n\nEXPOSE 8002                               # Document listening port\n\nENV FLASK_APP=src.aiservice\nENV FLASK_RUN_HOST=0.0.0.0\nENV FLASK_RUN_PORT=8002\nENV FLASK_ENV=production\n\nCMD [\"flask\", \"run\"]                      # Start Flask server\n</code></pre>"},{"location":"developer/docker/#manifest-engine-deployment-flow","title":"Manifest Engine Deployment Flow","text":"<p>When you click Deploy in Infra Hub:</p> <ol> <li>Build Image: Converts the ZIP from Artifactory into a Docker image using the Dockerfile in <code>scripts/</code></li> <li>Push to ACR: Uploads the image to Azure Container Registry under a predictable naming scheme</li> <li>Deploy to AKS: Spins up the container on Azure Kubernetes Service, wiring in the correct HTTP route (<code>/datafactory/{component}/{serviceName}</code>) from the manifest's component field</li> </ol>"},{"location":"developer/folder-structure/","title":"AI Service Folder Structure","text":"<p>Every AI Service follows this standardized folder structure:</p> <pre><code>my_ai_service/\n\u251c\u2500\u2500 auto_qa/           # Unit test code and test data\n\u2502   \u2514\u2500\u2500 service_unit_test.py\n\u251c\u2500\u2500 config/            # Configuration files and sample payloads\n\u2502   \u251c\u2500\u2500 input.json     # Input schema + UI configuration\n\u2502   \u251c\u2500\u2500 output.json    # Expected output sample\n\u2502   \u251c\u2500\u2500 feedback.json  # HIL review metadata\n\u2502   \u2514\u2500\u2500 env.config     # Environment-specific settings\n\u251c\u2500\u2500 doc/               # Human-friendly documentation\n\u2502   \u2514\u2500\u2500 note.txt       # Service overview, design rationale\n\u251c\u2500\u2500 lib/               # Python dependencies\n\u2502   \u2514\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 scripts/           # Deployment manifests\n\u2502   \u2514\u2500\u2500 manifest.xml   # Deployment blueprint\n\u251c\u2500\u2500 src/               # Core source code\n\u2502   \u251c\u2500\u2500 Abstract_Class.py      # Service contract / skeleton\n\u2502   \u251c\u2500\u2500 Concrete_Class.py      # Service-specific logic\n\u2502   \u251c\u2500\u2500 aiservice.py           # Context class / HTTP route\n\u2502   \u251c\u2500\u2500 environment.py         # Config management\n\u2502   \u2514\u2500\u2500 constants.py\n\u251c\u2500\u2500 unit_testing/      # Future integration/e2e tests\n\u251c\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 .gitignore\n</code></pre>"},{"location":"developer/folder-structure/#folder-details","title":"Folder Details","text":""},{"location":"developer/folder-structure/#auto_qa","title":"<code>auto_qa/</code>","text":"<p>Contains all code and data needed for automated unit testing. Allows developers to run checks locally to catch regressions before code leaves the workstation. Validates inputs, outputs, and error handling as part of every build.</p>"},{"location":"developer/folder-structure/#config","title":"<code>config/</code>","text":"<p>Contains configuration files and sample payloads. Key files:</p> <ul> <li><code>input.json</code> \u2014 Defines the structure, default values, and UI behavior of all input parameters. Contains <code>input_payload</code> (key-value pairs) and <code>payload_setup</code> (UI configuration: dropdowns, textboxes, validations). Used to auto-generate parameter forms in GenAI Studio and enable low-code pipeline setup.</li> <li><code>output.json</code> \u2014 Sample of the AI service's response after successful execution. Includes status, message, KPIs (execution time, processed records), and output metadata.</li> <li><code>feedback.json</code> \u2014 Supports human-in-the-loop review by capturing reviewer comments, required adjustments, and approval metadata.</li> </ul>"},{"location":"developer/folder-structure/#doc","title":"<code>doc/</code>","text":"<p>High-level, human-friendly documentation about the service. Onboarding notes, design rationale, troubleshooting tips. Versioned alongside source code for seamless knowledge transfer.</p>"},{"location":"developer/folder-structure/#lib","title":"<code>lib/</code>","text":"<p>All external dependencies as <code>requirements.txt</code>. Running <code>pip install -r lib/requirements.txt</code> instantly sets up the exact environment. Avoids version conflicts and ensures reproducible builds.</p>"},{"location":"developer/folder-structure/#scripts","title":"<code>scripts/</code>","text":"<p>Deployment assets and orchestration definitions. The <code>manifest.xml</code> specifies how the service is packaged, where it runs, and which endpoints it exposes. Evolves alongside the code in version control.</p>"},{"location":"developer/folder-structure/#src","title":"<code>src/</code>","text":"<p>Core of the service: abstract classes, concrete implementations, and context/strategy classes. Also defines the HTTP route and integrates with config, secret stores, and logging.</p>"},{"location":"developer/folder-structure/#unit_testing","title":"<code>unit_testing/</code>","text":"<p>Currently a placeholder. Reserved for broader integration tests, smoke tests, performance benchmarks, and API integration scenarios.</p>"},{"location":"developer/logging/","title":"Logging","text":"<p>AI Services in AIDF support two complementary log types:</p>"},{"location":"developer/logging/#1-verbose-logs-structured","title":"1. Verbose Logs (Structured)","text":"<p>Structured, high-level execution entries stored in the database (<code>AI_SERVICE_EXECUTION_LOGS</code>) and visible in the pipeline UI. Each entry includes:</p> <ul> <li>Timestamps for start/end of each step</li> <li>Total duration</li> <li>Number of input/output records</li> <li>Status (success/failure)</li> <li>Error messages, if any</li> </ul> <p>Useful for tracking service performance, auditing, and reviewing processed data summaries.</p>"},{"location":"developer/logging/#2-console-logs-real-time","title":"2. Console Logs (Real-time)","text":"<p>Real-time messages printed to terminal/log stream for development and debugging:</p> <pre><code>logger.info(\"TTS model loaded successfully\")        # High-level milestones\nlogger.debug(f\"Intermediate value: {variable}\")      # Debug variables\nlogger.error(f\"Exception: {str(e)}\")                 # Error traces\n</code></pre> <p>Rule</p> <p>Every function must be wrapped in a <code>try/except</code> block. Log exceptions with <code>logger.error()</code> before raising or returning an error response.</p>"},{"location":"developer/manifest/","title":"Manifest File (manifest.xml)","text":"<p>The manifest is the deployment blueprint for each AI Service. It centralizes all deployment metadata so that any change \u2014 new version, different sizing, extra databases \u2014 happens in one file and is versioned with the code.</p>"},{"location":"developer/manifest/#structure","title":"Structure","text":"<pre><code>&lt;Package&gt;\n  &lt;ArtifactInfo&gt;\n    &lt;Name&gt;suspicious_activity_detection&lt;/Name&gt;        &lt;!-- Must match ZIP filename --&gt;\n    &lt;DisplayName&gt;Suspicious Activity Detection&lt;/DisplayName&gt;\n    &lt;Description&gt;Detects suspicious activity from person tracks&lt;/Description&gt;\n    &lt;Module_Name&gt;DataFactory&lt;/Module_Name&gt;\n    &lt;Component&gt;Preprocessing&lt;/Component&gt;\n    &lt;AIServiceVersion&gt;1.0&lt;/AIServiceVersion&gt;\n    &lt;EnvironmentConfig&gt;config/env.config&lt;/EnvironmentConfig&gt;\n    &lt;InputPayLoad&gt;config/input.json&lt;/InputPayLoad&gt;\n    &lt;OutputResponse&gt;config/output.json&lt;/OutputResponse&gt;\n    &lt;Feedback&gt;config/feedback.json&lt;/Feedback&gt;\n    &lt;metainfo industry=\"Security\" collection=\"Vision\" /&gt;\n  &lt;/ArtifactInfo&gt;\n\n  &lt;Configuration&gt;\n    &lt;Deployment type=\"Algo\" technology=\"Azure\"\n                route=\"/datafactory/preprocessing/suspicious_activity\"\n                cluster=\"shared\"&gt;\n      &lt;DevSize cpu=\"2\" memory=\"4Gi\" DR=\"no\" /&gt;\n      &lt;QaSize cpu=\"4\" memory=\"8Gi\" DR=\"no\" /&gt;\n      &lt;StagingSize cpu=\"8\" memory=\"16Gi\" DR=\"yes\" /&gt;\n      &lt;ProdSize cpu=\"16\" memory=\"32Gi\" DR=\"yes\" /&gt;\n    &lt;/Deployment&gt;\n\n    &lt;DBServices&gt;\n      &lt;DBService type=\"SQL\" provider=\"AzureSQL\"&gt;\n        &lt;DevSize tier=\"Basic\" /&gt;\n        &lt;ProdSize tier=\"GeneralPurpose\" /&gt;\n      &lt;/DBService&gt;\n    &lt;/DBServices&gt;\n  &lt;/Configuration&gt;\n&lt;/Package&gt;\n</code></pre>"},{"location":"developer/manifest/#key-fields","title":"Key Fields","text":"Field Purpose <code>Name</code> Unique machine-readable ID. Must match the ZIP filename exactly. <code>Module_Name</code> Platform module (e.g., DataFactory). Visible in UI navigation. <code>Component</code> Sub-category (Ingestion, Preprocessing, Storage, etc.) <code>route</code> HTTP endpoint path registered in the API gateway <code>type=\"Algo\"</code> Azure-deployed algorithmic compute. Use <code>type=\"Data\"</code> for Databricks. <code>cluster=\"shared\"</code> Use shared Kubernetes cluster. Use <code>\"exclusive\"</code> for dedicated compute. <code>DevSize/QaSize/StagingSize/ProdSize</code> VM/container resource allocation per environment <code>DR=\"yes\"</code> Enable disaster-recovery replicas for staging/production"},{"location":"developer/onboarding/","title":"AI Service Onboarding","text":"<p>After building and testing your AI Service locally, follow these steps to onboard it into AIDF:</p>"},{"location":"developer/onboarding/#onboarding-steps","title":"Onboarding Steps","text":"<ol> <li>Role Switch \u2014 Assume the Platform Admin role in Infra Hub to manage service packaging and deployment</li> <li>Code Packaging \u2014 Organize code per AIDF folder structure \u2192 Create a ZIP archive of the entire service directory</li> <li>Publish to Artifactory \u2014 Use the Agile Workbench VS Code plugin to upload the ZIP to the Public Artifactory (Azure Blob container)</li> <li>Set Visibility \u2014 Choose Private (project-only) or Public (shared across workspace)</li> <li>Deploy via Manifest Engine \u2014 Click Deploy \u2192 Manifest Engine builds Docker image \u2192 pushes to ACR \u2192 deploys to AKS</li> <li>Test \u2014 Run a preliminary test with a sample payload to verify the service works correctly</li> <li>Admin Approval \u2014 A reviewer approves the service in My Reviews \u2014 only then is it available for pipeline use</li> </ol>"},{"location":"developer/onboarding/#deployment-flow-detail","title":"Deployment Flow Detail","text":"<p>When you click Deploy in Infra Hub:</p> <ol> <li>Build Image: Converts the ZIP from Artifactory into a Docker image using the Dockerfile in <code>scripts/</code></li> <li>Push to ACR: Uploads the image to Azure Container Registry under a predictable naming scheme</li> <li>Deploy to AKS: Spins up the container on Azure Kubernetes Service, wiring in the correct HTTP route (<code>/datafactory/{component}/{serviceName}</code>) from the manifest's component field</li> </ol>"},{"location":"developer/strategy-pattern/","title":"Strategy Design Pattern","text":"<p>All AI Services implement a strategy design pattern combining abstract base classes with factory logic to produce consistent, modular, extensible service components.</p>"},{"location":"developer/strategy-pattern/#three-layers","title":"Three Layers","text":""},{"location":"developer/strategy-pattern/#layer-1-abstract-base-class","title":"Layer 1: Abstract Base Class","text":"<p>The contract / skeleton. Defines what every service must implement:</p> <ul> <li>Pre-defined abstract methods for ingestion, preprocessing, and target operations</li> <li>Acts as a backbone \u2014 any service must implement these methods</li> <li>Example: <code>DF_PRP_Abstract_Class.py</code></li> </ul> <pre><code>class AbstractAIService:\n    @abstractmethod\n    def ingest(self, payload): ...\n\n    @abstractmethod\n    def preprocess(self, data): ...\n\n    @abstractmethod\n    def execute(self, data): ...\n\n    @abstractmethod\n    def on_finish(self, result): ...\n\n    @abstractmethod\n    def on_error(self, error): ...\n</code></pre>"},{"location":"developer/strategy-pattern/#layer-2-concrete-implementation","title":"Layer 2: Concrete Implementation","text":"<p>Fills in the AI-specific logic for each method. Each subclass only implements the template methods for its specific task. May include extra helper functions (e.g., secret-vault connectors, storage clients).</p> <pre><code>class SuspiciousActivityDetectionService(AbstractAIService):\n    def ingest(self, payload):\n        # Download person tracks JSON from Azure Blob\n        ...\n    def execute(self, data):\n        # Analyze keypoints and classify occlusion levels\n        ...\n</code></pre>"},{"location":"developer/strategy-pattern/#layer-3-contextstrategy-class-predefined-flow","title":"Layer 3: Context/Strategy Class (\"Predefined Flow\")","text":"<p>Orchestrates the lifecycle. Acts as a factory that:</p> <ol> <li>Reads configuration to determine which concrete class to instantiate</li> <li>Creates the object and invokes methods in order: <code>execute \u2192 on_finish</code> (or <code>on_error</code>)</li> <li>Defines the HTTP route and registers the endpoint</li> <li>Wraps every function in try/except blocks to properly log and raise errors</li> </ol> <pre><code>class AIServiceContext:\n    def __init__(self, config):\n        self.service = ConcreteServiceFactory.create(config)\n\n    def run(self, payload):\n        try:\n            data = self.service.ingest(payload)\n            result = self.service.execute(data)\n            return self.service.on_finish(result)\n        except Exception as e:\n            return self.service.on_error(e)\n\n# HTTP endpoint registration\n@app.route(\"/datafactory/preprocessing/suspicious_activity\", methods=[\"POST\"])\ndef handle():\n    return AIServiceContext(config).run(request.json)\n</code></pre>"},{"location":"developer/strategy-pattern/#extending-the-pattern","title":"Extending the Pattern","text":"<p>Adding a new AI Service capability:</p> <ol> <li>Create a new concrete class extending the abstract base</li> <li>Register it in the context class factory</li> <li>No changes needed to existing logic or other services</li> </ol>"},{"location":"infra-hub/","title":"Infra Hub","text":"<p>Infra Hub is the infrastructure provisioning and management engine of AIDF. It abstracts the complexity of setting up cloud resources \u2014 virtual machines, Kubernetes clusters, storage accounts \u2014 so every AI service can acquire the right compute at deployment time.</p> <p>Currently supports: Azure, Databricks, RunPod, Denvr, on-premises data centers.</p> <p>Roadmap: AWS, GCP, Alibaba Cloud, NVIDIA DGX appliances.</p>"},{"location":"infra-hub/#two-views","title":"Two Views","text":"View Purpose Key Sections Admin View Manage workspaces, projects, HIL tasks Dashboard, Workspaces, Projects, HIL Tasks Config View Manage AI services, pipelines, approvals Dashboard, HIL Tasks, AI Services, Pipeline Template, My Reviews <p>Use the top-left dropdown to switch between views.</p>"},{"location":"infra-hub/#navigation","title":"Navigation","text":"<ul> <li>Workspaces \u2014 Top-level cloud account/region containers</li> <li>Projects &amp; Environments \u2014 Scoped containers for AI service development</li> <li>Managing AI Services \u2014 Deploy, test, approve, and manage AI Services</li> <li>Pipeline Templates \u2014 Build and promote reusable workflow templates</li> <li>HIL Tasks \u2014 Human-in-the-Loop task management</li> <li>Roles &amp; Permissions \u2014 User role definitions</li> </ul>"},{"location":"infra-hub/ai-services/","title":"Managing AI Services in Infra Hub","text":"<p>AI Services are the fundamental building blocks of AI intelligence in AIDF. They are categorized into three types:</p> <ul> <li>Data Services: Data processing, storage, and transformation</li> <li>Model Services: ML/AI model capabilities (inference, fine-tuning)</li> <li>Application Services: End-to-end AI applications</li> </ul>"},{"location":"infra-hub/ai-services/#service-visibility","title":"Service Visibility","text":"Visibility Description Public Available as a shared library across all projects in the workspace. Added to the common library. Can be licensed. Private Available only within the specific project. Used for proofs-of-concept or proprietary services."},{"location":"infra-hub/ai-services/#adding-an-ai-service","title":"Adding an AI Service","text":"<ol> <li>Switch to Config view</li> <li>Click AI Services \u2192 <code>+Add</code></li> <li>Select visibility (Public/Private)</li> <li>Select the service ZIP file (must be uploaded to Azure Public Artifactory first)</li> <li>Click Submit \u2014 the Manifest Engine automatically builds and deploys the service</li> <li>A reviewer must approve the service before it can be used in pipelines</li> </ol>"},{"location":"infra-hub/ai-services/#ai-service-management-options","title":"AI Service Management Options","text":"Action Effect Test Run a validation test with a sample payload before approving Deploy Deploy a public service to the current project Update Replace current version; old version is automatically undeployed Undeploy Stop the service and release all allocated resources Delete Completely remove the service and perform full resource cleanup"},{"location":"infra-hub/hil-tasks/","title":"Human-in-the-Loop (HIL) Tasks","text":"<p>HIL Tasks are activities where human expertise is intentionally integrated into AI workflows to improve accuracy, reliability, and adaptability. The HIL Tasks section in Admin View displays all tasks created through the self-service platform.</p>"},{"location":"infra-hub/hil-tasks/#capabilities","title":"Capabilities","text":"<ul> <li>Specify conditions for human intervention</li> <li>Use natural language to define feedback requirements</li> <li>Integrate with annotation platforms (e.g., Label Studio, custom apps in Data Canvas)</li> <li>Monitor task completion status from one centralized view</li> </ul>"},{"location":"infra-hub/hil-tasks/#hil-in-the-annotation-workflow","title":"HIL in the Annotation Workflow","text":"<p>HIL tasks are typically created when:</p> <ol> <li>A Data Hub Self-Service Request (SSR) is submitted for annotation</li> <li>An AI pipeline flags results with low confidence for human review</li> <li>An Agent Workbench safety/governance evaluation requires expert validation</li> </ol> <p>See Data Hub for the full annotation workflow.</p>"},{"location":"infra-hub/pipeline-templates/","title":"Pipeline Templates","text":"<p>An AI pipeline is an automated, end-to-end workflow that transforms raw data into actionable AI outputs. A template is the reusable definition; an instance is a live execution of that template processing real data.</p>"},{"location":"infra-hub/pipeline-templates/#key-principles","title":"Key Principles","text":"<ul> <li>Modularity: Each component independently deployable and testable</li> <li>Reproducibility: Every run traceable and reproducible</li> <li>Scalability: Architecture handles varying data volumes and computational demands</li> <li>Reliability: Built-in error handling and recovery mechanisms</li> <li>Observability: Comprehensive monitoring and logging throughout</li> </ul>"},{"location":"infra-hub/pipeline-templates/#creating-a-pipeline-template","title":"Creating a Pipeline Template","text":"<ol> <li>Switch to Config view \u2192 click Pipeline Template \u2192 <code>+Pipeline</code></li> <li>Select the component and sub-component for each service</li> <li>Drag and drop AI services onto the canvas; link from the Start point</li> <li>Connect services to create logical flow; double-click arrows to add conditions</li> <li>Configure each node (double-click \u2192 fill parameters \u2192 click Set)</li> <li>Enter pipeline name, description, and category \u2192 click Create</li> <li>Test the pipeline \u2192 submit for approval in My Reviews</li> <li>After approval, the template becomes available in GenAI Studio as an instance</li> </ol>"},{"location":"infra-hub/pipeline-templates/#example-yolo-video-auto-labeling-pipeline","title":"Example: YOLO Video Auto-Labeling Pipeline","text":"<p>Identifies objects in videos and auto-annotates them, feeding undetected objects to human annotators.</p> <pre><code>Pipeline Flow:\n  Start\n    \u2192 YOLO Object Detection (Pre-processing)\n        Source: Azure Blob Container\n        Model: YOLOv8 / YOLOv9 / YOLOv10\n        Output Format: YOLO + COCO\n        Storage Type: Physical (pushes to Data Marketplace)\n  End\n</code></pre>"},{"location":"infra-hub/pipeline-templates/#example-vila-15b-video-context-extraction","title":"Example: VILA 15B Video Context Extraction","text":"<p>NVIDIA's VILA 15B model analyzes video at frame level, generating contextual insights based on a natural-language prompt.</p> <pre><code>Pipeline Flow:\n  Start\n    \u2192 Villa15-B-version2 (Pre-processing)\n        Source: Azure Blob Container\n        Prompt: \"Identify all safety hazards and describe what is happening\"\n        SAS Token: Required for Azure Blob access\n        Output: JSON context summary per video\n  End\n</code></pre>"},{"location":"infra-hub/pipeline-templates/#manager-controller-bulk-load-pipelines","title":"Manager Controller &amp; Bulk Load Pipelines","text":"<p>Two special system pipelines manage the Human-in-the-Loop annotation flow:</p> <ul> <li>Manager_Controller: Exports completed annotation task data from the annotation platform (using Task IDs) to configured storage and updates SSR status in AIDF.</li> <li>Bulk_Load: Imports exported annotated metadata into Data Hub, making it available for downstream consumption. Configurable by industry, usage, nature, storage type, and dataset category.</li> </ul>"},{"location":"infra-hub/projects/","title":"Projects &amp; Environments","text":"<p>A project is a scoped container for AI service development, pipelines, and related artifacts. Projects must be created inside an existing workspace, inheriting its region, environment tag, and resource boundaries.</p>"},{"location":"infra-hub/projects/#creating-a-project","title":"Creating a Project","text":"<ol> <li>In Infra Hub, click Projects \u2192 <code>+ Project</code></li> <li>Enter project name and description</li> <li>Select the parent workspace</li> <li>Assign users and roles (Platform Admin, AI Engineer, QA, Architect, Auditor)</li> <li>Select environment(s) to link to the project</li> <li>Submit</li> </ol>"},{"location":"infra-hub/projects/#environment-types","title":"Environment Types","text":"Environment Purpose Characteristics Development (Dev) Feature development, experiments, initial debugging Smallest compute footprint. Rapid, unchecked deployments permitted. Quality Assurance (QA) Systematic testing, validation, integration checks Medium instances. Functional, regression, and integration test suites. Staging Near-production replica, load testing, user acceptance Matches production architecture. Same container images as Prod but isolated from live traffic. Production (Prod) Live environment serving real users Largest compute allocation. Full disaster-recovery and scaling policies. Only approved service versions deployed. <p>Info</p> <p>The environment within a workspace is shared across all projects in that workspace. If a workspace has a Medium environment, all projects under it share that compute pool.</p>"},{"location":"infra-hub/projects/#switching-projects","title":"Switching Projects","text":"<p>Use the project switcher (top-left dropdown) \u2192 select Config view \u2192 click the switch icon \u2192 select the desired project and environment \u2192 Submit.</p>"},{"location":"infra-hub/rbac/","title":"User Roles &amp; Permissions","text":"Role Description Platform Admin Full access to all applications and features. Can create/delete workspaces and projects. Can approve AI services and pipelines. AI Engineer Access to AI development tools, AI services, pipeline creation, and model workbench. QA Access to quality assurance features, testing pipelines. Architect Access to architectural components and system design views. Auditor Read-only access to auditing features and compliance reports. <p>Current state</p> <p>All users currently have access to all applications regardless of role. Full Role-Based Access Control (RBAC) with tailored application visibility is planned for a future release.</p>"},{"location":"infra-hub/workspaces/","title":"Workspaces","text":"<p>A workspace is the top-level logical boundary in AIDF. It defines both the cloud account/region and the environment stage (Dev, QA, Staging, Prod). All projects, resources, and AI services live inside a workspace.</p>"},{"location":"infra-hub/workspaces/#when-to-create-workspaces","title":"When to Create Workspaces","text":"<ul> <li>One workspace per business unit (e.g., MUFG GCMD team, Legal team, Marketing)</li> <li>One workspace per country/region (e.g., Singapore, International)</li> <li>One workspace per major AI initiative (e.g., Customer Insight Generation)</li> </ul>"},{"location":"infra-hub/workspaces/#creating-a-workspace","title":"Creating a Workspace","text":"<ol> <li>Navigate to Workspaces tab \u2192 click <code>+Workspace</code></li> <li>Choose provider: Amazon Web Services, Microsoft Azure, Google Cloud Platform, NVIDIA NGC</li> <li>Enter Identity Details: Admin user email (triggers approval workflow \u2014 only authorized personnel can create workspaces)</li> <li>Workspace Details: Name, description, cloud account, region</li> <li>Add Environment(s): Must add at least one. Choose type (Dev, QA, Stage, Prod) and size (Small/Medium/Large)<ul> <li>Small: Light development, minimal data, low-end Kubernetes pod</li> <li>Medium: QA/staging, mirrors production performance without full scale</li> <li>Large: Production, high-throughput, multi-region HA clusters</li> </ul> </li> <li>Submit for provisioning</li> </ol> <p>Dynamic resource allocation</p> <p>Creating an environment defines a quota (max GPU units, CPU cores, memory) without committing physical hardware. Resources are allocated from the shared pool only when a workload actively runs \u2014 optimizing cost by paying only for active computation.</p>"},{"location":"infra-hub/workspaces/#workspace-lifecycle","title":"Workspace Lifecycle","text":"<ul> <li>Edit: Change description, add/modify environments</li> <li>Delete: Automatically deallocates ALL child projects, infrastructure (clusters, VMs, storage), and Kubernetes namespaces \u2014 preventing \"zombie\" resource costs</li> </ul>"},{"location":"platform/agent-workbench/","title":"Agent Workbench","text":"<p>Agent Workbench is the most recent module in AIDF. Its objective is to enable building, importing, validating, and monitoring agentic LLM workflows (LangGraph, custom multi-agent systems) for business process automation.</p>"},{"location":"platform/agent-workbench/#creating-agent-workflows","title":"Creating Agent Workflows","text":"<p>There are multiple entry points:</p> <ul> <li>Natural language description: A business user describes the workflow in plain English; the platform generates the agent workflow graph.</li> <li>Import existing workflow: AI engineers who have built LangGraph or similar workflows outside AIDF can import them.</li> <li>Map to AI services: Each workflow node is mapped to an onboarded AI service; engineers wire up the code for each node.</li> </ul>"},{"location":"platform/agent-workbench/#why-import-an-already-built-workflow","title":"Why Import an Already-Built Workflow?","text":"<p>Even if a workflow was built outside AIDF, importing it enables:</p> <ol> <li>Validation runs \u2014 measure accuracy against ground truth datasets</li> <li>Safety evaluation \u2014 test robustness against malicious inputs (prompt injection, adversarial data)</li> <li>Governance compliance \u2014 check adherence to corporate/country AI governance policies</li> <li>Iterative improvement tracking \u2014 history of all validation runs with logs and metrics</li> <li>Production promotion \u2014 only workflows with acceptable metrics are promoted to production</li> </ol>"},{"location":"platform/agent-workbench/#case-study-mufg-corporate-onboarding-workflow","title":"Case Study: MUFG Corporate Onboarding Workflow","text":"<p>Business Problem: MUFG relationship managers must manually produce 45+ KYC/onboarding documents after interviewing a corporate customer, then process signed documents back into the banking system.</p> <p>The Agent Workflow:</p> <ol> <li>Relationship manager uploads interview sheet (Excel/CSV) to the web app</li> <li>Node 1 (Completeness Check): AI validates whether all required interview questions were answered</li> <li>Node 2 (Form Generation): Generates 45 compliance and KYC documents from interview data</li> <li>Node 3 (Document Dispatch): Sends documents to the end customer for digital/wet signature</li> <li>Customer returns signed documents; relationship manager uploads them</li> <li>Node 4 (Internet Form Generation): Produces internal banking forms from signed documents</li> <li>Node 5 (API Submission): Submits forms to MUFG banking system via API to onboard the customer</li> </ol>"},{"location":"platform/agent-workbench/#validation-evaluation","title":"Validation &amp; Evaluation","text":"<p>In Agent Workbench, AI engineers can run multi-pipeline evaluation reports:</p> <ul> <li>Accuracy report: Compare generated form fields against expected ground truth</li> <li>Coverage report: Percentage of required fields correctly populated</li> <li>Security/safety report: Test response to injected malicious content (e.g., \"How do I prepare a bomb?\" embedded in the interview sheet)</li> <li>Governance report: Check adherence to Singapore AI governance rules, MUFG corporate AI policy</li> <li>History: All validation run iterations are stored with logs, execution status, and metric trends</li> </ul> <p>Note</p> <p>This demo was shown to multiple financial sector customers. MUFG was very close to signing a pilot at the time of training.</p>"},{"location":"platform/ai-workbench/","title":"AI Workbench","text":"<p>AI Workbench is the platform's model catalog and development environment. It holds every AI model onboarded as an \"AI Service of type model\" \u2014 LLMs, CNNs, custom algorithms, ASR engines, and more.</p>"},{"location":"platform/ai-workbench/#1-model-benchmarking","title":"1. Model Benchmarking","text":"<p>Before choosing a model for a use case, AI engineers run benchmarking to identify the best performer:</p> <ul> <li>Generic benchmarking: Use publicly available or domain-specific ground truth datasets. Compare multiple models (e.g., Claude Sonnet, Gemini, GPT-4o) on metrics like accuracy, latency, and cost.</li> <li>Use-case specific benchmarking: Create a custom ground truth dataset in Data Hub specific to your use case (e.g., legal covenant extraction samples). Run a new benchmark to see which model performs best on your actual data.</li> <li>Results produce comparison reports showing metric scores across all selected models.</li> </ul> <p>Best practice</p> <p>Use generic benchmarking to eliminate clearly underperforming models, then run use-case specific benchmarking with your curated ground truth to make the final selection.</p>"},{"location":"platform/ai-workbench/#2-model-fine-tuning","title":"2. Model Fine-Tuning","text":"<p>When a model's off-the-shelf accuracy is insufficient:</p> <ol> <li>Select the model to fine-tune (e.g., Whisper Large for ASR)</li> <li>Choose the compute platform and cluster (RunPod, Denvr, Azure)</li> <li>Select fine-tuning aspects:<ul> <li>Core performance \u2014 general accuracy improvement</li> <li>Robustness \u2014 performance across acoustic noise levels, environments</li> <li>Ethical alignment \u2014 bias mitigation, demographic fairness</li> </ul> </li> <li>Upload datasets per aspect to Data Hub; specify train/test split (e.g., 70/30)</li> <li>Run the fine-tuning job; view iterative reports with metrics like Word Error Rate (WER), Character Error Rate (CER), BLEU scores</li> <li>Compare multiple fine-tuning run results side-by-side including infrastructure consumption</li> </ol>"},{"location":"platform/ai-workbench/#3-ai-governance","title":"3. AI Governance","text":"<p>Before deploying a model, generate governance reports against organizational policies:</p> <ul> <li>Upload governance policy documents (e.g., Singapore AI regulations, MUFG corporate AI policy)</li> <li>Select frameworks: Atlas, NIST AI RMF (more to be added)</li> <li>Create one or more policies (country-level, industry-level, corporate-level)</li> <li>Run an assessment \u2014 generates a scorecard showing model compliance with each policy</li> <li>All governance reports are stored centrally per model version</li> </ul>"},{"location":"platform/ai-workbench/#the-ai-workbench-workflow","title":"The AI Workbench Workflow","text":"<pre><code>Benchmark \u2192 Fine-Tune \u2192 Evaluate \u2192 Governance \u2192 Baseline &amp; Promote\n</code></pre> <ol> <li>Benchmark \u2014 Identify the best candidate model for your domain</li> <li>Fine-Tune \u2014 Improve accuracy on your specific use case data</li> <li>Evaluate \u2014 Compare metrics across fine-tuning iterations</li> <li>Governance \u2014 Run policy assessments and generate compliance scorecards</li> <li>Baseline &amp; Promote \u2014 Lock the validated model version for inferencing/production</li> </ol>"},{"location":"platform/data-hub/","title":"Data Hub","text":"<p>Data Hub acts as the enterprise data catalog and metadata store. Every time an AI service processes or generates a data artifact, the service registers that output in Data Hub. It is the single source of truth for all data within AIDF.</p>"},{"location":"platform/data-hub/#key-capabilities","title":"Key Capabilities","text":"<ul> <li>Data Dictionary: Type, size, format, and schema for each data element.</li> <li>Metadata Management: Rich metadata including tags, categories, and custom attributes.</li> <li>Lineage Tracking: Full provenance \u2014 where the data came from (Data Marketplace, One Data Platform, external upload), what transformations were applied (PII suppression, encoding, cleansing), and where it is being consumed (AI Workbench training, GenAI Studio pipeline).</li> <li>Human-in-the-Loop (HIL) Integration: Trigger annotation tasks directly from Data Hub; completed annotations flow back automatically from Data Canvas.</li> <li>Synthetic Data Generation: Create synthetic datasets using LLMs with configurable prompts, model selection, quality-check models, and generation guidelines.</li> <li>Self-Service Requests: Select data elements and create annotation tasks (e.g., Audio Transcription, Video Annotation) that are published to the Data Canvas annotation platform.</li> </ul>"},{"location":"platform/data-hub/#data-ingestion-sources","title":"Data Ingestion Sources","text":"Source Description Data Marketplace 3,000+ curated, labeled datasets. One Data Platform Collect new data via crowd resources (images, video, audio). External Upload Upload from Azure Blob Store via an automated pipeline. (Direct upload from local machine is a planned feature pending data compliance review.) Pipeline Output Any AI service output can be registered directly into Data Hub."},{"location":"platform/data-hub/#annotation-workflow","title":"Annotation Workflow","text":"<pre><code>1. Select data elements in Data Hub\n   \u2514\u2500 Choose individual files or a batch that require annotation\n\n2. Create a Self-Service Request (SSR)\n   \u2514\u2500 Select annotation template (Audio Transcription, Custom App from Data Canvas)\n\n3. Task auto-published to Data Canvas\n   \u2514\u2500 Human annotators receive the task with annotation guidelines\n\n4. Annotator completes the task\n   \u2514\u2500 Annotated data (ground truth labels) is submitted back\n\n5. Run Manager Controller Pipeline\n   \u2514\u2500 Exports completed task data from annotation platform back to AIDF storage\n\n6. Run Bulk Load Pipeline\n   \u2514\u2500 Imports annotated metadata into Data Hub; lineage updated\n</code></pre>"},{"location":"platform/data-hub/#pre-annotation-with-genai-studio-pipelines","title":"Pre-Annotation with GenAI Studio Pipelines","text":"<p>Rather than sending raw data to humans, you can first run a GenAI Studio pipeline for automated pre-annotation, dramatically reducing annotator workload.</p> <p>Example \u2014 Isaac Snapdragon Pre-Annotation Pipeline (built for Qualcomm):</p> <ol> <li>Pull 800+ dark video files from Azure Blob Store</li> <li>Apply brightness/contrast preprocessing</li> <li>Run people detection, upper body, and head detection services</li> <li>Detect PII, sensitive topics, and perform redaction</li> <li>Push pre-annotated results to human annotators via HIL task</li> </ol> <p>Result</p> <p>Humans only need to verify or correct pre-existing labels rather than annotating from scratch, cutting annotation time significantly.</p>"},{"location":"platform/genai-studio/","title":"GenAI Studio","text":"<p>GenAI Studio is the central hub for managing AI services and building pipelines. It reads each service's manifest to register API endpoints, allocate compute, and enforce visibility settings. It streamlines the entire deployment lifecycle from smoke test to production.</p>"},{"location":"platform/genai-studio/#pipeline-builder","title":"Pipeline Builder","text":"<p>The drag-and-drop canvas lets you chain multiple AI services into end-to-end workflows:</p> <ul> <li>Select AI services from the component library (Data Factory, Feature Extraction, Content Generation, etc.)</li> <li>Drop services onto the canvas and connect them with arrows</li> <li>Configure each node: input/output parameters, model to use, storage configuration</li> <li>Add conditional branching: define conditions on output KPIs \u2014 e.g., \"only proceed to PII redaction if sensitive topics are detected\"</li> <li>Connect a Human-in-the-Loop node to push results to Data Canvas for expert review</li> </ul>"},{"location":"platform/genai-studio/#pipeline-types","title":"Pipeline Types","text":"Type Purpose Example Pre-annotation Automate label creation before human review YOLO object detection \u2192 upper body detection \u2192 HIL task Data processing Transform, clean, and enrich data Audio ingestion \u2192 noise reduction \u2192 speaker diarization \u2192 PII redaction Post-annotation Validate and consolidate annotated data Quality check \u2192 Data Hub registration RAG/Inference Production AI inference pipelines Document extraction \u2192 LLM extraction \u2192 structured output"},{"location":"platform/genai-studio/#real-world-example-isaac-snapdragon-pre-annotation-pipeline","title":"Real-World Example: Isaac Snapdragon Pre-Annotation Pipeline","text":"<p>Built for Qualcomm's Isaac camera project, this pipeline processes dark security footage:</p> <ol> <li>Pull 800+ video files from Azure Blob Store</li> <li>Brightness/contrast preprocessing (making dark footage legible)</li> <li>People detection service</li> <li>Upper body and head detection</li> <li>Plant and object detection</li> <li>Aggregate detection outputs</li> <li>PII detection and redaction (replace with beeps)</li> <li>Sensitive topic detection</li> <li>Push to HIL task for human verification</li> </ol> <p>Result</p> <p>Humans only verify/correct pre-existing detections rather than labeling from scratch on near-invisible footage.</p>"},{"location":"platform/genai-studio/#example-yolo-video-auto-labeling-pipeline","title":"Example: YOLO Video Auto-Labeling Pipeline","text":"<pre><code>Start\n  \u2192 YOLO Object Detection (Pre-processing)\n      Source: Azure Blob Container\n      Model: YOLOv8 / YOLOv9 / YOLOv10\n      Output Format: YOLO + COCO\n      Storage Type: Physical (pushes to Data Marketplace)\nEnd\n</code></pre>"},{"location":"platform/genai-studio/#example-vila-15b-video-context-extraction","title":"Example: VILA 15B Video Context Extraction","text":"<p>NVIDIA's VILA 15B model analyzes video at frame level, generating contextual insights based on a natural-language prompt. Detects objects, actions, hazards, and interprets scenes.</p> <pre><code>Start\n  \u2192 Villa15-B-version2 (Pre-processing)\n      Source: Azure Blob Container\n      Prompt: \"Identify all safety hazards and describe what is happening\"\n      SAS Token: Required for Azure Blob access\n      Output: JSON context summary per video\nEnd\n</code></pre>"},{"location":"platform/genai-studio/#manager-controller-bulk-load-pipelines","title":"Manager Controller &amp; Bulk Load Pipelines","text":"<p>Two special system pipelines manage the Human-in-the-Loop annotation flow:</p> <ul> <li>Manager_Controller: Exports completed annotation task data from the annotation platform (using Task IDs) to configured storage and updates SSR status in AIDF.</li> <li>Bulk_Load: Imports exported annotated metadata into Data Hub, making it available for downstream consumption. Configurable by industry, usage, nature, storage type, and dataset category.</li> </ul>"},{"location":"platform/genbi/","title":"GenBI (Cognitive Analytics)","text":"<p>GenBI is a natural-language analytics layer that lets anyone ask plain-English questions against structured data and receive concise trend analyses. It removes the need for SQL or BI tool knowledge to extract insights from enterprise data.</p> <p>Example: \"Show me monthly sales growth over the past year\" \u2192 GenBI queries the connected data source and returns a trend analysis with visualization data.</p>"},{"location":"platform/genbi/#key-features","title":"Key Features","text":"<ul> <li>Ask questions in plain English against structured enterprise data</li> <li>No SQL or BI tool knowledge required</li> <li>Returns trend analyses and insight summaries</li> <li>Integrates with data sources registered in Data Hub</li> </ul>"},{"location":"platform/genbi/#use-cases","title":"Use Cases","text":"<ul> <li>Business analytics and reporting without technical expertise</li> <li>Executive dashboards driven by natural-language queries</li> <li>Ad-hoc data exploration for non-technical stakeholders</li> <li>Feeding insights into GenAI Studio pipelines for automated reporting</li> </ul>"},{"location":"reference/phi4-service/","title":"Microsoft Phi-4 Fine-Tuning Service","text":"<p>A complete AI Service implementation for fine-tuning Microsoft Phi-4 language models using distributed GPU computing. Demonstrates the full AIDF AI Service pattern for model-type services.</p> <p>Location</p> <p>Code directory: <code>Code/microsoftphi4reference/</code> Main service: <code>src/microsoftphi4.py</code> (36 KB) Fine-tuning factory: <code>src/ModelFinetuningFactory.py</code> (239 KB)</p>"},{"location":"reference/phi4-service/#features","title":"Features","text":"<ul> <li>Distributed GPU Computing: RunPod integration for on-demand GPU infrastructure. Pay-per-use model. Parallel fine-tuning jobs across multiple GPUs.</li> <li>Advanced Model Training: HuggingFace Transformers integration. Transfer learning with custom datasets. Memory optimization strategies. Periodic checkpointing for recovery.</li> <li>Comprehensive Monitoring: Real-time metrics (training loss, validation loss, accuracy, learning rate). MLFlow integration for experiment logging and visualization. WER, CER, BLEU scores, and perplexity measurements.</li> <li>Automated Data Pipeline: DataMarket integration for dataset acquisition. Container deployment via ACR to RunPod. Central model repository with versioning.</li> </ul>"},{"location":"reference/phi4-service/#service-workflow","title":"Service Workflow","text":"<pre><code>1. Container Image Deployment\n   Pull Docker image from Azure Container Registry \u2192 Deploy to RunPod GPU infrastructure\n\n2. Dataset Acquisition\n   Integrate with DataMarket platform \u2192 Download dataset by unique identifier\n\n3. Model Training Pipeline\n   Load Phi-4 model \u2192 Configure hyperparameters and GPU utilization \u2192 Run distributed training\n\n4. Monitoring &amp; Evaluation\n   Real-time metrics capture \u2192 MLFlow logging \u2192 Multi-metric performance analysis\n\n5. Storage &amp; Reporting\n   Database integration \u2192 Version-controlled model storage \u2192 Automated report generation\n</code></pre>"},{"location":"reference/phi4-service/#configuration-fields","title":"Configuration Fields","text":"<pre><code>Config.HOST_URL                        # Service host URL\nConfig.ACR_NAME                        # Azure Container Registry name\nConfig.RUNPOD_API_KEY                  # RunPod API Key\nConfig.REGISTRY_AUTH_ID_RUNPOD         # Registry auth for RunPod\nConfig.TENANT_ID / CLIENT_ID / CLIENT_SECRET  # Azure AD credentials\nConfig.DB_CONNECTION_STRING            # Primary database\nConfig.CENTRAL_MODEL_REPO_CONNECTION_STRING   # Model artifact storage\nConfig.ML_FLOW_AZURE_CONNECTION_STRING # MLFlow Azure backend\n</code></pre>"},{"location":"reference/phi4-service/#key-source-files","title":"Key Source Files","text":"File Purpose Size <code>src/microsoftphi4.py</code> Main service entry point, HTTP route, context orchestration 36 KB <code>src/ModelFinetuningFactory.py</code> Complete fine-tuning pipeline \u2014 data loading, training loops, checkpointing, evaluation 239 KB <code>src/HuggingFaceModelFactory.py</code> Model download, configuration, and initialization from HuggingFace Hub 25 KB <code>src/rlhf.py</code> Reinforcement Learning from Human Feedback implementation 27 KB <code>src/ollama_infer.py</code> Local inference via Ollama runtime 26 KB <code>src/datasheet_info_fetch_and_insert.py</code> Dataset metadata management and insertion 47 KB <code>src/api_data_access_layer.py</code> Database CRUD operations for training runs and results 17 KB <code>resources/model_queries.py</code> SQL query templates for model metadata operations 12 KB <code>config/ai_llm_model_finetuning.json</code> Full fine-tuning request schema with hyperparameters, compute config, evaluation settings 15 KB"},{"location":"reference/reference-template/","title":"Reference Template","text":"<p>The <code>reference_template</code> project is the canonical starting point for building any new AI Service on AIDF. Copy this template, rename it, and implement your service-specific logic in the concrete class.</p>"},{"location":"reference/reference-template/#structure","title":"Structure","text":"<pre><code>reference_template/\n\u251c\u2500\u2500 auto_qa/\n\u2502   \u2514\u2500\u2500 template_unit_test.py    # 7.7 KB \u2014 unit test scaffold\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 input.json               # Input schema template\n\u2502   \u251c\u2500\u2500 output.json              # Output schema template\n\u2502   \u251c\u2500\u2500 feedback.json            # HIL feedback template\n\u2502   \u2514\u2500\u2500 env.config               # Environment configuration\n\u251c\u2500\u2500 lib/\n\u2502   \u2514\u2500\u2500 requirements.txt         # Base dependencies\n\u251c\u2500\u2500 scripts/\n\u2502   \u2514\u2500\u2500 manifest.xml             # Deployment blueprint template\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 Abstract_Class.py        # Abstract base class skeleton\n\u2502   \u251c\u2500\u2500 aiservice.py             # Context class + HTTP route (23 KB)\n\u2502   \u251c\u2500\u2500 environment.py           # Config management\n\u2502   \u251c\u2500\u2500 constants.py             # Shared constants\n\u2502   \u251c\u2500\u2500 api_data_access_layer.py # Database access layer\n\u2502   \u2514\u2500\u2500 compute_offloading/     # Multi-platform compute\n\u2502       \u251c\u2500\u2500 abstract_class.py    # Compute offloading contract\n\u2502       \u251c\u2500\u2500 azure_deploy.py      # Azure deployment\n\u2502       \u251c\u2500\u2500 denvr_deploy.py      # Denvr GPU cluster deployment (13 KB)\n\u2502       \u251c\u2500\u2500 on_prem.py           # On-premises deployment (41 KB)\n\u2502       \u251c\u2500\u2500 runpod_deploy.py     # RunPod GPU deployment (11 KB)\n\u2502       \u2514\u2500\u2500 platform_cluster_manager.py  # Cluster orchestration\n\u2514\u2500\u2500 Dockerfile\n</code></pre>"},{"location":"reference/reference-template/#compute-offloading","title":"Compute Offloading","text":"<p>The <code>compute_offloading</code> module enables heavy compute jobs (training, inference on large models) to run on external GPU infrastructure while the main service container remains lightweight:</p> Platform Use Case File RunPod On-demand GPU pods, pay-per-use. Ideal for fine-tuning jobs. <code>runpod_deploy.py</code> Denvr High-performance GPU clusters. Suitable for large-scale training. <code>denvr_deploy.py</code> On-Premises Internal GPU servers for data-sovereign workloads. <code>on_prem.py</code> Azure Azure VMs with GPU support via AKS node pools. <code>azure_deploy.py</code>"},{"location":"reference/suspicious-activity/","title":"Suspicious Activity Detection Service","text":"<p>A vision pipeline AI service that analyzes person detection results (person tracks) to generate Suspicious Activity (SA) annotations based on visible keypoint counts. A post-processing module that works on top of people detection output \u2014 no ML model inference required.</p> <p>Location</p> <p>Code directory: <code>Code/Suspicious_Activity/</code> Version: 1.0.0 Created: December 22, 2025</p>"},{"location":"reference/suspicious-activity/#detection-logic","title":"Detection Logic","text":"<p>Classifies occlusion levels based on the number of visible body keypoints:</p> Level Name Keypoints Visible Level 0 Fully Visible 15\u201317 keypoints Level 1 Partly Occluded 10\u201314 keypoints Level 2 Largely Occluded 5\u20139 keypoints Skipped Too Occluded &lt;5 keypoints"},{"location":"reference/suspicious-activity/#processing-pipeline","title":"Processing Pipeline","text":"<pre><code>1. API receives request\n   Flask endpoint with JSON payload including storage config and video file paths\n\n2. Download person tracks JSON\n   For each input video, download the corresponding video_id.json from configured storage\n\n3. Submit SA detection job\n   Uses Ray actor pattern for parallel processing across multiple videos\n\n4. Analyze keypoints per frame\n   For each person per frame, count visible keypoints \u2192 classify occlusion level\n\n5. Generate SA annotation JSON\n   Aggregate per-frame results with statistics and upload to target storage\n</code></pre>"},{"location":"reference/suspicious-activity/#output-json-structure","title":"Output JSON Structure","text":"<pre><code>{\n  \"success\": true,\n  \"detection_type\": \"people_suspicious_activity\",\n  \"video_info\": {\n    \"video_id\": \"video1\",\n    \"video_metadata\": { \"resolution\": \"1920x1080\", \"fps\": 30.0, \"total_frames\": 900 }\n  },\n  \"configuration\": {\n    \"occlusion_levels\": {\n      \"level_0\": \"15-17 keypoints (fully visible)\",\n      \"level_1\": \"10-14 keypoints (partly occluded)\",\n      \"level_2\": \"5-9 keypoints (largely occluded)\"\n    }\n  },\n  \"summary\": {\n    \"total_persons_processed\": 150,\n    \"total_sa_detections\": 120,\n    \"too_occluded_skipped\": 30,\n    \"detection_rate\": 80.0\n  }\n}\n</code></pre>"},{"location":"reference/suspicious-activity/#ray-integration","title":"Ray Integration","text":"<p>The service uses Ray for distributed, parallel execution:</p> <ul> <li>Configure <code>enable_ray: Y</code> to use Ray cluster for parallel processing</li> <li>Uses Actor pattern for stateful, persistent processing across video batches</li> <li><code>ray_address</code>: Connect to an existing cluster or start a new one</li> <li><code>ray_task_num_cpus</code>: Control CPU allocation per task (default: 1)</li> </ul>"},{"location":"reference/suspicious-activity/#deployment","title":"Deployment","text":"<pre><code>docker build -t suspicious-activity:1.0 .\ndocker run -p 8002:8002 -e ENVIRONMENT=Development suspicious-activity:1.0\n\n# Endpoint:\nPOST /api/&lt;aiservices&gt;/&lt;centific&gt;/loop/model/inference/suspicious_activity_detection\n</code></pre>"}]}
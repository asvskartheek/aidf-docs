# Human-in-the-Loop (HIL) Tasks

HIL Tasks are activities where human expertise is intentionally integrated into AI workflows to improve accuracy, reliability, and adaptability. The HIL Tasks section in Admin View displays all tasks created through the self-service platform.

## Capabilities

- Specify conditions for human intervention
- Use natural language to define feedback requirements
- Integrate with annotation platforms (e.g., Label Studio, custom apps in Data Canvas)
- Monitor task completion status from one centralized view

## HIL in the Annotation Workflow

HIL tasks are typically created when:

1. A Data Hub Self-Service Request (SSR) is submitted for annotation
2. An AI pipeline flags results with low confidence for human review
3. An Agent Workbench safety/governance evaluation requires expert validation

See [Data Hub](../platform/data-hub.md) for the full annotation workflow.
